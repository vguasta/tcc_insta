{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c216a87c",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d7fe35",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3563d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeaffedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to import moviepy. Need only for Video upload.\n"
     ]
    }
   ],
   "source": [
    "from data.InstagramAPI.InstagramAPI import InstagramAPI\n",
    "from data.plots import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bff01e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths #pip install opencv-python\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.applications import VGG16, imagenet_utils\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bea1de",
   "metadata": {},
   "source": [
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24220d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On Anaconda Prompt:\n",
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e27de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests_toolbelt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22ba638d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-1.3.0-cp39-cp39-win_amd64.whl (10.2 MB)\n",
      "Requirement already satisfied: pytz>=2017.3 in d:\\ic\\venv\\envtcc\\lib\\site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in d:\\ic\\venv\\envtcc\\lib\\site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\ic\\venv\\envtcc\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\ic\\venv\\envtcc\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0069b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37835719",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02279c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5d6962",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f66ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE = \"guastafriends\"\n",
    "\n",
    "DATE_BEGIN = datetime(2021,1,1,0,0).timestamp()\n",
    "\n",
    "PATH_BASE                   = os.getcwd()\n",
    "PATH_BASE_DATA              = os.path.join(PATH_BASE, \"data\")\n",
    "PATH_1_CREATING_DATASET     = os.path.join(PATH_BASE_DATA, \"1-Creating_Dataset\")\n",
    "PATH_FIRST_DATASET          = os.path.join(PATH_1_CREATING_DATASET, \"Dataset\")\n",
    "PATH_FIRST_DATASET_TYPE     = os.path.join(PATH_FIRST_DATASET, TYPE)\n",
    "PATH_2_PREPARING_DATASET    = os.path.join(PATH_BASE_DATA, \"2-Preparing_Dataset\")\n",
    "PATH_CLEAN_DATASET          = os.path.join(PATH_2_PREPARING_DATASET, \"Clean_Dataset\")\n",
    "PATH_CLEAN_DATASET_TYPE     = os.path.join(PATH_CLEAN_DATASET, TYPE)\n",
    "PATH_3_PROCESSING_DATASET   = os.path.join(PATH_BASE_DATA, \"3-Processing_Dataset\")\n",
    "PATH_SIMILARY_ACCOUNTS      = os.path.join(PATH_3_PROCESSING_DATASET, \"Similary_Accounts\")\n",
    "PATH_SIMILARY_ACCOUNTS_TYPE = os.path.join(PATH_SIMILARY_ACCOUNTS, TYPE)\n",
    "PATH_LABEL_COUNTER          = os.path.join(PATH_3_PROCESSING_DATASET, \"Label_Counter\")\n",
    "PATH_LABEL_COUNTER_TYPE     = os.path.join(PATH_LABEL_COUNTER, TYPE)\n",
    "PATH_4_CLASSIFYING_DATASET  = os.path.join(PATH_BASE_DATA, \"4-Classifying_Dataset\")\n",
    "\n",
    "MEDIA_TYPES = [1] #8\n",
    "CANDIDATE = -1\n",
    "#CANDIDATE = 0\n",
    "#CANDIDATE = 1\n",
    "#CANDIDATE = 2\n",
    "\n",
    "ONLY_WITH_COMMENTS = True\n",
    "\n",
    "#FOLLOWERS_RADIUS = 3\n",
    "#LIKES_RADIUS = 95\n",
    "\n",
    "FOLLOWERS_RADIUS = 100\n",
    "LIKES_RADIUS = 0\n",
    "\n",
    "RANDOM_IGUAL = True\n",
    "#RANDOM_IGUAL = False\n",
    "\n",
    "BATCH_LABEL_COUNTER = 7500\n",
    "\n",
    "BATCH_SIZE = 60\n",
    "CV = 10\n",
    "\n",
    "print(f\"DATE_BEGIN                  : {DATE_BEGIN}\")\n",
    "print(f\"PATH_BASE                   : {PATH_BASE}\")\n",
    "print(f\"PATH_BASE_DATA              : {PATH_BASE_DATA}\")\n",
    "print(f\"PATH_1_CREATING_DATASET     : {PATH_1_CREATING_DATASET}\")\n",
    "print(f\"PATH_FIRST_DATASET          : {PATH_FIRST_DATASET}\")\n",
    "print(f\"PATH_FIRST_DATASET_TYPE     : {PATH_FIRST_DATASET_TYPE}\")\n",
    "print(f\"PATH_2_PREPARING_DATASET    : {PATH_2_PREPARING_DATASET}\")\n",
    "print(f\"PATH_CLEAN_DATASET          : {PATH_CLEAN_DATASET}\")\n",
    "print(f\"PATH_CLEAN_DATASET_TYPE     : {PATH_CLEAN_DATASET_TYPE}\")\n",
    "print(f\"PATH_3_PROCESSING_DATASET   : {PATH_3_PROCESSING_DATASET}\")\n",
    "print(f\"PATH_SIMILARY_ACCOUNTS      : {PATH_SIMILARY_ACCOUNTS}\")\n",
    "print(f\"PATH_SIMILARY_ACCOUNTS_TYPE : {PATH_SIMILARY_ACCOUNTS_TYPE}\")\n",
    "print(f\"PATH_LABEL_COUNTER          : {PATH_LABEL_COUNTER}\")\n",
    "print(f\"PATH_LABEL_COUNTER_TYPE     : {PATH_LABEL_COUNTER_TYPE}\")\n",
    "print(f\"PATH_4_CLASSIFYING_DATASET  : {PATH_4_CLASSIFYING_DATASET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a68d5",
   "metadata": {},
   "source": [
    "## Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07557f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(path_file, data, access_mode='a'):\n",
    "    file = open(path_file, access_mode)\n",
    "    if isinstance(data, list):\n",
    "        for d in data:\n",
    "            file.write(d)\n",
    "    else:\n",
    "        file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "def read_file(path_file, access_mode='r'):\n",
    "    file = open(path_file, access_mode)\n",
    "    data = file.readlines()\n",
    "    file.close()  \n",
    "    return data\n",
    "\n",
    "def write_json(json_path, data, access_mode='w', encoding=\"utf-8\"):\n",
    "    with open(json_path, access_mode, encoding=encoding) as json_file:\n",
    "        json.dump(data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "def read_json(json_path, access_mode='r', encoding=\"utf-8\"):\n",
    "    with open(json_path, access_mode, encoding=encoding) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "##############################################\n",
    "\n",
    "def get_accounts():\n",
    "    list_accounts = read_file(os.path.join(PATH_1_CREATING_DATASET, f\"{TYPE}_users.txt\"))\n",
    "    return [s.strip() for s in list_accounts]\n",
    "\n",
    "def log_print(data):\n",
    "    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {data}\", flush=True)\n",
    "\n",
    "def create_path(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        log_print(f\"Path '{path}' was created.\")\n",
    "\n",
    "def delete_path(path):\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path, ignore_errors=True)\n",
    "        log_print(f\"Path '{path}' was deleted.\")\n",
    "\n",
    "def random_sleep(min=0.1, max=5):\n",
    "    time.sleep(round(random.uniform(min, max), 1))\n",
    "\n",
    "##############################################\n",
    "\n",
    "def loop_images_extract_features(imagePaths, labels, csv):\n",
    "    # loop over the images in batches\n",
    "    for (b, i) in enumerate(range(0, len(imagePaths), BATCH_SIZE)):\n",
    "        # extract the batch of images and labels, then initialize the\n",
    "        # list of actual images that will be passed through the network\n",
    "        # for feature extraction\n",
    "        log_print(\"[INFO] processing batch {}/{}\".format(b + 1, int(np.ceil(len(imagePaths) / float(BATCH_SIZE)))))\n",
    "        batchPaths = imagePaths[i:i + BATCH_SIZE]\n",
    "        batchLabels = le.transform(labels[i:i + BATCH_SIZE])\n",
    "        batchImages = []\n",
    "\n",
    "        # loop over the images and labels in the current batch\n",
    "        for imagePath in batchPaths:\n",
    "            # load the input image using the Keras helper utility while ensuring the image is resized to 224x224 pixels\n",
    "            image = load_img(imagePath, target_size=(224, 224))\n",
    "            image = img_to_array(image)\n",
    "\n",
    "            # preprocess the image by (1) expanding the dimensions and\n",
    "            # (2) subtracting the mean RGB pixel intensity from the ImageNet dataset\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            image = imagenet_utils.preprocess_input(image)\n",
    "\n",
    "            # add the image to the batch\n",
    "            batchImages.append(image)\n",
    "\n",
    "        # pass the images through the network and use the outputs as\n",
    "        # our actual features, then reshape the features into a\n",
    "        # flattened volume\n",
    "        batchImages = np.vstack(batchImages)\n",
    "        features = model.predict(batchImages, batch_size=BATCH_SIZE)\n",
    "        features = features.reshape((features.shape[0], 7 * 7 * 512))\n",
    "\n",
    "        # loop over the class labels and extracted features\n",
    "        for (label, vec) in zip(batchLabels, features):\n",
    "            # construct a row that exists of the class label and\n",
    "            # extracted features\n",
    "            vec = \",\".join([str(v) for v in vec])\n",
    "            csv.write(f\"{label},{vec}\\n\")\n",
    "            \n",
    "def load_data_split(splitPath):\n",
    "    # initialize the data and labels\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # loop over the rows in the data split file\n",
    "    for row in open(splitPath):\n",
    "        # extract the class label and features from the row\n",
    "        row = row.strip().split(\",\")\n",
    "        label = row[0]\n",
    "        features = np.array(row[1:], dtype=\"float\")\n",
    "\n",
    "        # update the data and label lists\n",
    "        data.append(features)\n",
    "        labels.append(label)\n",
    "\n",
    "    # convert the data and labels to NumPy arrays\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # return a tuple of the data and labels\n",
    "    return (data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c0dc79",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Reset Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243eadbe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "delete_path(PATH_FIRST_DATASET_TYPE)\n",
    "delete_path(PATH_CLEAN_DATASET_TYPE)\n",
    "delete_path(PATH_SIMILARY_ACCOUNTS_TYPE)\n",
    "delete_path(PATH_LABEL_COUNTER_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0394b3",
   "metadata": {},
   "source": [
    "### Checking Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c724cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_path(PATH_BASE_DATA)\n",
    "#create_path(PATH_1_CREATING_DATASET)\n",
    "#create_path(PATH_FIRST_DATASET)\n",
    "create_path(PATH_FIRST_DATASET_TYPE)\n",
    "#create_path(PATH_2_PREPARING_DATASET)\n",
    "#create_path(PATH_CLEAN_DATASET)\n",
    "create_path(PATH_CLEAN_DATASET_TYPE)\n",
    "#create_path(PATH_3_PROCESSING_DATASET)\n",
    "#create_path(PATH_SIMILARY_ACCOUNTS)\n",
    "create_path(PATH_SIMILARY_ACCOUNTS_TYPE)\n",
    "#create_path(PATH_LABEL_COUNTER)\n",
    "create_path(PATH_LABEL_COUNTER_TYPE)\n",
    "create_path(PATH_4_CLASSIFYING_DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e5bd94",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Creating Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db82974",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53978827",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file_logins = read_json(os.path.join(PATH_1_CREATING_DATASET, \"logins.json\"))\n",
    "\n",
    "def error_log(acc, log_type, e):\n",
    "    log_print(f\"Failed to process {acc} account: {str(e)}\")\n",
    "    log_print(f\"Appending to file {TYPE}_users_error.log\")\n",
    "    write_file(os.path.join(PATH_1_CREATING_DATASET, f\"{TYPE}_users_error.log\"), \\\n",
    "               [f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {log_type} - {acc} - {str(e)}\", \"\\n\"])\n",
    "    print(\"**********************************************\\n\")\n",
    "        \n",
    "def get_login(id_login=0, logins=file_logins[\"Logins\"]):\n",
    "    api = None\n",
    "    logged = False\n",
    "    while not logged:      \n",
    "        log_print(\"Trying login in account: \" + logins[id_login][\"username\"])\n",
    "        api = InstagramAPI(logins[id_login][\"username\"], logins[id_login][\"password\"])\n",
    "        logged = api.login()\n",
    "        if not logged:\n",
    "            id_login += 1\n",
    "            if id_login >= len(logins):\n",
    "                id_login = 0\n",
    "            random_sleep()\n",
    "    return (api, id_login)\n",
    "        \n",
    "def search_username(api, id_login, acc):\n",
    "    log_print(f\"Searching username {acc}\")\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        api.searchUsername(acc)\n",
    "\n",
    "        log_print(f\"searchUsername-status_code: {api.LastResponse.status_code}\")\n",
    "        if api.LastResponse.status_code == 429 or ('message' in api.LastJson and api.LastJson[\"message\"] == 'challenge_required'):\n",
    "            (api, id_login) = get_login(id_login=id_login+1)\n",
    "        elif api.LastResponse.status_code != 200:\n",
    "            log_print(f\"LastResponse: {api.LastResponse}\")\n",
    "            log_print(f\"LastJson: {api.LastJson}\")\n",
    "            raise Exception(f\"{api.LastResponse.status_code}-{api.LastJson}\")\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        random_sleep()\n",
    "    \n",
    "    return (api, id_login)\n",
    "\n",
    "def create_file_user_info(api, id_login, acc):\n",
    "    \n",
    "    (api, id_login) = search_username(api, id_login, acc)\n",
    "    \n",
    "    imgs_user = {}\n",
    "    imgs_user[\"user\"] = acc\n",
    "    imgs_user[\"downloaded\"] = 0\n",
    "\n",
    "    create_path(os.path.join(PATH_FIRST_DATASET_TYPE, acc))\n",
    "    user = api.LastJson[\"user\"]\n",
    "    \n",
    "    log_print(\"Creating file user_info\")\n",
    "    user[\"collected_at\"] = datetime.timestamp(datetime.now())\n",
    "    \n",
    "    write_json(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"user_info.json\"), user)\n",
    "    log_print(\"File user_info created\")\n",
    "    \n",
    "    return (api, id_login, user)\n",
    "\n",
    "def get_user_feed(api, id_login, user, next_max_id=None):  \n",
    "    \n",
    "    while True:   \n",
    "        if next_max_id == None:\n",
    "            api.getUserFeed(user['pk'], minTimestamp=int(DATE_BEGIN))\n",
    "        else:\n",
    "            api.getUserFeed(user['pk'], maxid=next_max_id, minTimestamp=int(DATE_BEGIN))\n",
    "\n",
    "        log_print(f\"getUserFeed-status_code: {api.LastResponse.status_code}\")\n",
    "        if api.LastResponse.status_code == 429 or ('message' in api.LastJson and api.LastJson[\"message\"] == 'challenge_required'):\n",
    "            (api, id_login) = get_login(id_login=id_login+1)\n",
    "        elif api.LastResponse.status_code != 200:\n",
    "            log_print(f\"LastResponse: {api.LastResponse}\")\n",
    "            log_print(f\"LastJson: {api.LastJson}\")\n",
    "            raise Exception(f\"{api.LastResponse.status_code}-{api.LastJson}\")\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "        random_sleep()\n",
    "    \n",
    "    return (api, id_login)\n",
    "\n",
    "def create_file_user_posts(api, id_login, user):\n",
    "    log_print(\"Creating file user_posts\")\n",
    "    (api, id_login) = get_user_feed(api, id_login, user)\n",
    "        \n",
    "    posts = api.LastJson\n",
    "\n",
    "    while (posts[\"more_available\"]):\n",
    "        random_sleep()\n",
    "        aux_bool = False\n",
    "        (api, id_login) = get_user_feed(api, id_login, user, posts[\"next_max_id\"])\n",
    "\n",
    "        posts[\"items\"].extend(api.LastJson[\"items\"])\n",
    "        posts[\"num_results\"] += api.LastJson[\"num_results\"]\n",
    "        posts[\"more_available\"] =  api.LastJson[\"more_available\"]\n",
    "        if posts[\"more_available\"]:\n",
    "            posts[\"next_max_id\"] = api.LastJson[\"next_max_id\"]\n",
    "        else:\n",
    "            del posts['next_max_id']\n",
    "        posts[\"auto_load_more_enabled\"] = api.LastJson[\"auto_load_more_enabled\"]\n",
    "        posts[\"status\"] = api.LastJson[\"status\"]\n",
    "        \n",
    "    posts[\"collected_at\"] = datetime.timestamp(datetime.now())\n",
    "    write_json(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"user_posts.json\"), posts)\n",
    "    log_print(\"File user_posts created\")\n",
    "    \n",
    "    return (api, id_login, posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908fbc73",
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(PATH_1_CREATING_DATASET, f\"{TYPE}_users.bkp\")):\n",
    "    shutil.copyfile(os.path.join(PATH_1_CREATING_DATASET, f\"{TYPE}_users.txt\"), os.path.join(PATH_1_CREATING_DATASET, f\"{TYPE}_users.bkp\"))\n",
    "\n",
    "(api, id_login) = get_login()\n",
    "\n",
    "log_print(\"Type: \" + TYPE)\n",
    "log_print(\"Starting...\\n\")\n",
    "\n",
    "list_accounts = get_accounts()\n",
    "for acc in list_accounts:\n",
    "    try:\n",
    "        \n",
    "        (api, id_login, user) = create_file_user_info(api, id_login, acc)\n",
    "        (api, id_login, posts) = create_file_user_posts(api, id_login, user)\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_log(acc, \"GettingData\", e)\n",
    "    else:\n",
    "        log_print(f\"Account {acc} successfully processed\")\n",
    "        print(\"**********************************************\\n\")\n",
    "\n",
    "log_print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db1fa5c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Creating Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2b582",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_file_user_posts_overview(posts):\n",
    "    \n",
    "    log_print(\"Creating file user_posts_overview\")\n",
    "    \n",
    "    media_count_total = 0\n",
    "    posts_overview = {}\n",
    "    posts_overview[\"collected_at\"] = posts[\"collected_at\"]\n",
    "    posts_overview[\"num_results\"] = posts[\"num_results\"]\n",
    "    posts_overview[\"items\"] = []\n",
    "    \n",
    "    for post in posts[\"items\"]:\n",
    "        \n",
    "        if not post[\"media_type\"] in MEDIA_TYPES:\n",
    "            continue\n",
    "        if ONLY_WITH_COMMENTS and \"comments_disabled\" in post:\n",
    "            continue\n",
    "            \n",
    "        post_overview = {}\n",
    "        post_overview[\"pk\"] = post[\"pk\"]\n",
    "        post_overview[\"id\"] = post[\"id\"]\n",
    "        post_overview[\"taken_at\"] = post[\"taken_at\"]\n",
    "        post_overview[\"taken_at_year\"] = datetime.fromtimestamp(post_overview[\"taken_at\"]).year\n",
    "\n",
    "        post_overview[\"caption_text\"] = \"\"\n",
    "        if not post[\"caption\"] is None:\n",
    "            post_overview[\"caption_text\"] = post[\"caption\"][\"text\"]\n",
    "\n",
    "        post_overview[\"media_type\"] = post[\"media_type\"]\n",
    "        post_overview[\"images\"] = []\n",
    "        if post_overview[\"media_type\"] == 1:\n",
    "            post_overview[\"media_count\"] = 1\n",
    "            \n",
    "            candidate_aux = CANDIDATE\n",
    "            while candidate_aux >= len(post[\"image_versions2\"][\"candidates\"]):\n",
    "                candidate_aux -= 1\n",
    "                \n",
    "            image = {}\n",
    "            image[\"pk\"] = post_overview[\"pk\"]\n",
    "            image[\"width\"] = post[\"image_versions2\"][\"candidates\"][candidate_aux][\"width\"]\n",
    "            image[\"height\"] = post[\"image_versions2\"][\"candidates\"][candidate_aux][\"height\"]\n",
    "            image[\"url\"] = post[\"image_versions2\"][\"candidates\"][candidate_aux][\"url\"]\n",
    "            post_overview[\"images\"].append(image)\n",
    "        elif post_overview[\"media_type\"] == 8:\n",
    "            post_overview[\"media_count\"] = post[\"carousel_media_count\"]\n",
    "            for carousel_post in post[\"carousel_media\"]:\n",
    "                \n",
    "                candidate_aux = CANDIDATE\n",
    "                while candidate_aux >= len(carousel_post[\"image_versions2\"][\"candidates\"]):\n",
    "                    candidate_aux -= 1\n",
    "                \n",
    "                image = {}\n",
    "                image[\"pk\"] = carousel_post[\"pk\"]\n",
    "                image[\"width\"] = carousel_post[\"image_versions2\"][\"candidates\"][candidate_aux][\"width\"]\n",
    "                image[\"height\"] = carousel_post[\"image_versions2\"][\"candidates\"][candidate_aux][\"height\"]\n",
    "                image[\"url\"] = carousel_post[\"image_versions2\"][\"candidates\"][candidate_aux][\"url\"]\n",
    "                post_overview[\"images\"].append(image)\n",
    "\n",
    "        media_count_total += post_overview[\"media_count\"]\n",
    "\n",
    "        post_overview[\"location\"] = None\n",
    "        if \"location\" in post:\n",
    "            post_overview[\"location\"] = post[\"location\"]\n",
    "\n",
    "        post_overview[\"like_count\"] = post[\"like_count\"]\n",
    "\n",
    "        post_overview[\"comments_disabled\"] = False\n",
    "        if \"comments_disabled\" in post:\n",
    "            post_overview[\"comments_disabled\"] = post[\"comments_disabled\"]\n",
    "        if post_overview[\"comments_disabled\"]:\n",
    "            post_overview[\"comment_count\"] = 0\n",
    "        else:\n",
    "            post_overview[\"comment_count\"] = post[\"comment_count\"]\n",
    "\n",
    "        post_overview[\"caption_text_length\"] = len(post_overview[\"caption_text\"])\n",
    "        hashtags = [word[1:] for word in post_overview[\"caption_text\"].split() if word[0] == '#']\n",
    "        post_overview[\"caption_hashtags_count\"] = len(hashtags)\n",
    "        post_overview[\"caption_hashtags\"] = []\n",
    "\n",
    "        if post_overview[\"caption_hashtags_count\"] > 0:\n",
    "            post_overview[\"caption_hashtags\"] = hashtags\n",
    "        post_overview[\"timestamp_duration\"] = posts_overview[\"collected_at\"] - post_overview[\"taken_at\"]\n",
    "        post_overview[\"likes_by_duration\"] = post_overview[\"like_count\"] / post_overview[\"timestamp_duration\"]\n",
    "        post_overview[\"comments_by_duration\"] = post_overview[\"comment_count\"] / post_overview[\"timestamp_duration\"]\n",
    "\n",
    "        posts_overview[\"items\"].append(post_overview)\n",
    "            \n",
    "    posts_overview[\"media_count_total\"] = media_count_total\n",
    "    write_json(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"user_posts_overview.json\"), posts_overview)\n",
    "    log_print(\"File user_posts_overview created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c1f7ba",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list_accounts = get_accounts()\n",
    "for acc in list_accounts:\n",
    "    \n",
    "    try:\n",
    "        log_print(f\"Processing account {acc}\")\n",
    "\n",
    "        posts = read_json(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"user_posts.json\"))\n",
    "\n",
    "        create_file_user_posts_overview(posts)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_log(acc, \"CreatingOverview\", e)\n",
    "    else:\n",
    "        log_print(f\"Account {acc} successfully processed\")\n",
    "        print(\"**********************************************\\n\")\n",
    "        \n",
    "log_print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ff5c8f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Downloading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f53157",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def download_image(url, path):\n",
    "    resp = requests.get(url, stream=True, timeout=20)  # Open the url image, set stream to True, this will return the stream content.\n",
    "    local_file = open(path, 'wb')  # Open a local file with wb ( write binary ) permission.\n",
    "    resp.raw.decode_content = True  # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "    shutil.copyfileobj(resp.raw, local_file)  # Copy the response stream raw data to local image file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d209d8cf",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "again = True\n",
    "while again:\n",
    "    again = False\n",
    "    list_accounts = get_accounts()\n",
    "    for acc in list_accounts:\n",
    "\n",
    "        try:\n",
    "            log_print(f\"Processing account {acc}\")\n",
    "            \n",
    "            create_path(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"images\"))\n",
    "                \n",
    "            posts_overview = read_json(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"user_posts_overview.json\"))\n",
    "\n",
    "            log_print(\"Downloading posts pictures:\")\n",
    "            imgs_user_downloaded = len(list(next(os.walk(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"images\")))[2])) - 1 #0 - root, 1 - dirs, 2 - files\n",
    "            if imgs_user_downloaded <= 0:\n",
    "                imgs_user_downloaded = 0\n",
    "            if imgs_user_downloaded < posts_overview['media_count_total']:\n",
    "                for post in posts_overview[\"items\"]:\n",
    "                    for image in post[\"images\"]:\n",
    "                        if not os.path.exists(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"images\", str(image[\"pk\"]) + \".jpg\")):\n",
    "                            download_image(image['url'], os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"images\", f\"{image['pk']}.jpg\"))\n",
    "                            imgs_user_downloaded += 1\n",
    "                            log_print(f\"{imgs_user_downloaded}/{posts_overview['media_count_total']}\")\n",
    "                \n",
    "            log_print(f\"{imgs_user_downloaded} image(s) downloaded\")\n",
    "        except Exception as e:\n",
    "            error_log(acc, \"DownloadingImages\", e)\n",
    "            again = True\n",
    "        else:\n",
    "            log_print(f\"Account {acc} successfully processed\")\n",
    "            print(\"**********************************************\\n\")\n",
    "            \n",
    "log_print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1b424e",
   "metadata": {},
   "source": [
    "# Preparing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149f7b09",
   "metadata": {},
   "source": [
    "1 - Depois de executar \"Checking Folders\":\n",
    "    Dentro da pasta \"CNN/CNN_Dataset/[nicho]\" você precisa inserir imagens para cada classe (pastas [nicho] e non_[nicho], por exemplo, dog e non_dog) dentro de cada pasta \"Train\" e \"Test\".\n",
    "    Infelizmente você precisa buscar de maneira manual imagens que pertencem e não pertencem ao nicho que você está estudando.\n",
    "    Recomenda-se pelo menos 1500 imagens para cada classe para o treinamento, totalizando 3000 imagens para o treinamento, e 500 imagens para cada classe para o teste, totalizando 1000 imagens para o teste.\n",
    "\n",
    "2 - O próximo passo é executar 3-Renaming_Images.py, para renomear cada imagem automaticamente, preparando-as para a CNN.\n",
    "\n",
    "3 - Executar 4-Extract_Features.py para extrair as características do conjunto de dados.\n",
    "\n",
    "4 - Executar 5-Train.py para treinar a CNN.\n",
    "\n",
    "5 - Com a CNN treinada, executar 6-Clean.py para classificar/limpar o conjunto de imagens do Instagram.\n",
    "\n",
    "6 - Por fim, executar 7-Moving_Images.py, para mover as imagens para o dataset limpo, em Clean_Dataset.\n",
    "\n",
    "7 - O Arquivo 8-Statistics.py trás algumas estatísticas sobre o dataset limpo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b554a25",
   "metadata": {},
   "source": [
    "## CNN Clean Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e7262",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_CNN_CLEAN_BASE                       = os.path.join(PATH_2_PREPARING_DATASET, \"CNN_Clean\")\n",
    "PATH_CNN_CLEAN_DATASET                    = os.path.join(PATH_CNN_CLEAN_BASE, \"CNN_Dataset\")\n",
    "PATH_CNN_CLEAN_DATASET_TYPE               = os.path.join(PATH_CNN_CLEAN_DATASET, TYPE)\n",
    "PATH_CNN_CLEAN_DATASET_TYPE_TRAIN         = os.path.join(PATH_CNN_CLEAN_DATASET_TYPE, \"Train\")\n",
    "PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_TYPE    = os.path.join(PATH_CNN_CLEAN_DATASET_TYPE_TRAIN, TYPE)\n",
    "PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_NONTYPE = os.path.join(PATH_CNN_CLEAN_DATASET_TYPE_TRAIN, f\"non_{TYPE}\")\n",
    "PATH_CNN_CLEAN_DATASET_TYPE_TEST          = os.path.join(PATH_CNN_CLEAN_DATASET_TYPE, \"Test\")\n",
    "PATH_CNN_CLEAN_DATASET_TYPE_TEST_TYPE     = os.path.join(PATH_CNN_CLEAN_DATASET_TYPE_TEST, TYPE)\n",
    "PATH_CNN_CLEAN_DATASET_TYPE_TEST_NONTYPE  = os.path.join(PATH_CNN_CLEAN_DATASET_TYPE_TEST, f\"non_{TYPE}\")\n",
    "PATH_CNN_CLEAN_OUTPUT                     = os.path.join(PATH_CNN_CLEAN_BASE, \"CNN_Output\")\n",
    "PATH_CNN_CLEAN_OUTPUT_TYPE                = os.path.join(PATH_CNN_CLEAN_OUTPUT, TYPE)\n",
    "\n",
    "print(f\"PATH_CNN_CLEAN_BASE                      : {PATH_CNN_CLEAN_BASE}\")\n",
    "print(f\"PATH_CNN_CLEAN_DATASET                   : {PATH_CNN_CLEAN_DATASET}\")\n",
    "print(f\"PATH_CNN_CLEAN_DATASET_TYPE              : {PATH_CNN_CLEAN_DATASET_TYPE}\")\n",
    "print(f\"PATH_CNN_CLEAN_DATASET_TYPE_TRAIN        : {PATH_CNN_CLEAN_DATASET_TYPE_TRAIN}\")\n",
    "print(f\"PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_TYPE   : {PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_TYPE}\")\n",
    "print(f\"PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_NONTYPE: {PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_NONTYPE}\")\n",
    "print(f\"PATH_CNN_CLEAN_DATASET_TYPE_TEST         : {PATH_CNN_CLEAN_DATASET_TYPE_TEST}\")\n",
    "print(f\"PATH_CNN_CLEAN_DATASET_TYPE_TEST_TYPE    : {PATH_CNN_CLEAN_DATASET_TYPE_TEST_TYPE}\")\n",
    "print(f\"PATH_CNN_CLEAN_DATASET_TYPE_TEST_NONTYPE : {PATH_CNN_CLEAN_DATASET_TYPE_TEST_NONTYPE}\")\n",
    "print(f\"PATH_CNN_CLEAN_OUTPUT                    : {PATH_CNN_CLEAN_OUTPUT}\")\n",
    "print(f\"PATH_CNN_CLEAN_OUTPUT_TYPE               : {PATH_CNN_CLEAN_OUTPUT_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6032c957",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Reset Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a61ab",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "delete_path(PATH_CNN_CLEAN_DATASET_TYPE)\n",
    "delete_path(PATH_CNN_CLEAN_OUTPUT_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af36d5",
   "metadata": {},
   "source": [
    "### Checking Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67af659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_path(PATH_CNN_CLEAN_BASE)\n",
    "#create_path(PATH_CNN_CLEAN_DATASET)\n",
    "#create_path(PATH_CNN_CLEAN_DATASET_TYPE)\n",
    "#create_path(PATH_CNN_CLEAN_DATASET_TYPE_TRAIN)\n",
    "create_path(PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_TYPE)\n",
    "create_path(PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_NONTYPE)\n",
    "#create_path(PATH_CNN_CLEAN_DATASET_TYPE_TEST)\n",
    "create_path(PATH_CNN_CLEAN_DATASET_TYPE_TEST_TYPE)\n",
    "create_path(PATH_CNN_CLEAN_DATASET_TYPE_TEST_NONTYPE)\n",
    "#create_path(PATH_CNN_CLEAN_OUTPUT)\n",
    "create_path(PATH_CNN_CLEAN_OUTPUT_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f81c580",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Renaming Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e46ae",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rename_images(img_list, file_path, prefix):\n",
    "    count = 0\n",
    "    for img in img_list:\n",
    "        os.rename(os.path.join(file_path, img), os.path.join(file_path, f\"{prefix}_{count}.jpg\"))\n",
    "        count += 1\n",
    "    \n",
    "train_type_images = next(os.walk(PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_TYPE))[2] #0 - root, 1 - dirs, 2 - files\n",
    "train_nontype_images = next(os.walk(PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_NONTYPE))[2] #0 - root, 1 - dirs, 2 - files\n",
    "test_type_images = next(os.walk(PATH_CNN_CLEAN_DATASET_TYPE_TEST_TYPE))[2] #0 - root, 1 - dirs, 2 - files\n",
    "test_nontype_images = next(os.walk(PATH_CNN_CLEAN_DATASET_TYPE_TEST_NONTYPE))[2] #0 - root, 1 - dirs, 2 - files\n",
    "\n",
    "rename_images(train_type_images, PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_TYPE, 1)\n",
    "rename_images(train_nontype_images, PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_NONTYPE, 0)\n",
    "rename_images(test_type_images, PATH_CNN_CLEAN_DATASET_TYPE_TEST_TYPE, 1)\n",
    "rename_images(test_nontype_images, PATH_CNN_CLEAN_DATASET_TYPE_TEST_NONTYPE, 0)\n",
    "\n",
    "log_print(\"All images has been renamed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204a4625",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2708030f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit([f\"non_{TYPE}\", TYPE])\n",
    "# serialize the label encoder to disk\n",
    "joblib.dump(le, os.path.join(PATH_CNN_CLEAN_OUTPUT_TYPE, \"le.pickle\"))\n",
    "\n",
    "# load the VGG16 network and initialize the label encoder\n",
    "log_print(\"[INFO] loading network...\")\n",
    "model = VGG16(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "# loop over the data splits\n",
    "for split in ([\"Train\", \"Test\"]):\n",
    "    # grab all image paths in the current split\n",
    "    log_print(f\"[INFO] processing '{split} split'...\")\n",
    "    p = os.path.join(PATH_CNN_CLEAN_DATASET_TYPE, split)\n",
    "    imagePaths = list(paths.list_images(p))\n",
    "\n",
    "    # randomly shuffle the image paths and then extract the class\n",
    "    # labels from the file paths\n",
    "    random.shuffle(imagePaths)\n",
    "\n",
    "    labels = [p.split(os.path.sep)[-2] for p in imagePaths]\n",
    "\n",
    "    # open the output CSV file for writing\n",
    "    csv = open(os.path.join(PATH_CNN_CLEAN_OUTPUT_TYPE, split + \".csv\"), \"w\")\n",
    "\n",
    "    loop_images_extract_features(imagePaths, labels, csv)\n",
    "\n",
    "    # close the CSV file\n",
    "    csv.close()\n",
    "log_print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dcf398",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84420153",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# derive the paths to the training and testing CSV files\n",
    "trainingPath = os.path.join(PATH_CNN_CLEAN_OUTPUT_TYPE, \"Train.csv\")\n",
    "testingPath = os.path.join(PATH_CNN_CLEAN_OUTPUT_TYPE, \"Test.csv\")\n",
    "\n",
    "# load the data from disk\n",
    "log_print(\"[INFO] loading data...\")\n",
    "(trainX, trainY) = load_data_split(trainingPath)\n",
    "(testX, testY) = load_data_split(testingPath)\n",
    "\n",
    "# load the label encoder from disk\n",
    "le = joblib.load(os.path.join(PATH_CNN_CLEAN_OUTPUT_TYPE, \"le.pickle\"))\n",
    "\n",
    "# train the model\n",
    "log_print(\"[INFO] training model...\")\n",
    "model = LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\")\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# evaluate the model\n",
    "log_print(\"[INFO] evaluating...\")\n",
    "preds = model.predict(testX)\n",
    "log_print(metrics.classification_report(testY, preds, target_names=le.classes_))\n",
    "plot_confusion_matrix(testY, preds, np.array(le.classes_), normalize=True)\n",
    "\n",
    "# serialize the model to disk\n",
    "log_print(\"[INFO] saving model...\")\n",
    "plt.savefig(os.path.join(PATH_CNN_CLEAN_OUTPUT_TYPE, \"test_normalized.svg\"))\n",
    "joblib.dump(model, os.path.join(PATH_CNN_CLEAN_OUTPUT_TYPE, \"model.pickle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57beee6a",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8752521",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img(\"D:\\IC\\\\tcc_insta\\data\\\\1-Creating_Dataset\\Dataset\\guastafriends\\joao.squinelato\\images\\\\2553959008324384891.jpg\", target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd995e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imagenet_utils.preprocess_input(np.expand_dims(img_to_array(img), axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e9dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 60\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca703a47",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the VGG16 network and initialize the label encoder\n",
    "log_print(\"[INFO] loading network...\")\n",
    "vgg16 = VGG16(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "model = joblib.load(os.path.join(PATH_CNN_CLEAN_OUTPUT_TYPE, \"model.pickle\"))\n",
    "\n",
    "list_accounts = get_accounts()\n",
    "\n",
    "for acc in list_accounts:\n",
    "\n",
    "    log_print(\"Processing account: \" + acc)\n",
    "\n",
    "    posts_overview = read_json(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"user_posts_overview.json\"))\n",
    "    df_posts_overview = None\n",
    "    df_posts_overview = pd.DataFrame(posts_overview[\"items\"])\n",
    "    df_posts_overview = df_posts_overview.apply(lambda x: [ (x[\"pk\"], image[\"pk\"], os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"images\", f\"{image['pk']}.jpg\"), image[\"url\"]) for image in x[\"images\"]], axis=1)\n",
    "    df_posts_overview = pd.DataFrame(list(itertools.chain.from_iterable(df_posts_overview.to_list())), columns=[\"pk_item\", \"pk_image\", \"path_image\", \"url\"])\n",
    "    \n",
    "    print(df_posts_overview)\n",
    "    tam = len(df_posts_overview.index)\n",
    "    for offset in range(0, tam, BATCH_SIZE):\n",
    "        log_print(f\"Batch {offset+1}/{round((tam/BATCH_SIZE)+0.5)}\")\n",
    "        log_print(\"Creating image_values\")\n",
    "        a = df_posts_overview[\"path_image\"].to_list()\n",
    "        print(imagenet_utils.preprocess_input(np.expand_dims(img_to_array(load_img(a[-1], target_size=(224, 224))), axis=0)))\n",
    "        df_posts_overview.loc[offset:offset+BATCH_SIZE,'image_values'] = df_batch.apply(lambda row: imagenet_utils.preprocess_input(np.expand_dims(img_to_array(load_img(row[\"path_image\"], target_size=(224, 224))), axis=0)), axis=1)\n",
    "        \n",
    "        print(df_posts_overview.loc[offset:offset+BATCH_SIZE,'image_values'])\n",
    "        \n",
    "        log_print(\"Predicting\")\n",
    "        batchImages = np.vstack(df_posts_overview.loc[offset:offset+BATCH_SIZE,'image_values'])\n",
    "        features = vgg16.predict(batchImages, batch_size=BATCH_SIZE)\n",
    "        features = features.reshape((features.shape[0], 7 * 7 * 512))\n",
    "        resul = model.predict(features)\n",
    "        df_posts_overview.loc[offset:offset+BATCH_SIZE,'predicted_label'] = resul\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a81b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_print(\"Images:\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "                imagePath = all_image_paths[j]\n",
    "\n",
    "                image_model = \n",
    "                image_model = \n",
    "\n",
    "                # preprocess the image by (1) expanding the dimensions and\n",
    "                # (2) subtracting the mean RGB pixel intensity from the\n",
    "                # ImageNet dataset\n",
    "                image_model = \n",
    "                image_model = imagenet_utils.preprocess_input(np.expand_dims(img_to_array(load_img(imagePath, target_size=(224, 224))), axis=0))\n",
    "\n",
    "                batchImages.append(image_model)\n",
    "        \n",
    "        batchImages = np.vstack(batchImages)\n",
    "        features = vgg16.predict(batchImages, batch_size=BATCH_SIZE)\n",
    "        features = features.reshape((features.shape[0], 7 * 7 * 512))\n",
    "\n",
    "        resul = model.predict(features)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        posts_overview_clean = {}\n",
    "        posts_overview_clean[\"collected_at\"] = posts_overview[\"collected_at\"]\n",
    "        posts_overview_clean[\"num_results\"] = 0\n",
    "        posts_overview_clean[\"items\"] = []\n",
    "        posts_overview_clean[\"media_count_total\"] = 0\n",
    "\n",
    "        \n",
    "        \n",
    "        for j in range(0, BATCH_SIZE):\n",
    "            soma = j+i\n",
    "            if soma <= tam:\n",
    "                if resul[j] == '0':\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            item_copy = item.copy()\n",
    "            item_copy[\"media_count\"] = 0\n",
    "            item_copy[\"images\"] = []\n",
    "\n",
    "                imagePath = os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"images\", str(all_images[j]['pk']) + \".jpg\")\n",
    "                log_print(str(count_image) + \"/\" + str(posts_overview[\"media_count_total\"]) + \" - \" + imagePath)\n",
    "                count_image += 1\n",
    "\n",
    "                try_count = 0\n",
    "                try_again = True\n",
    "                while try_again:\n",
    "                    try:\n",
    "                        \n",
    "                        \n",
    "\n",
    "                        \n",
    "\n",
    "                        if resul[0] == '0':\n",
    "                            log_print(TYPE + \"\\n\")\n",
    "                            item_copy[\"images\"].append(image)\n",
    "                            item_copy[\"media_count\"] += 1\n",
    "                            posts_overview_clean[\"media_count_total\"] += 1\n",
    "                        else:\n",
    "                            log_print(\"non_\" + TYPE + \"\\n\")\n",
    "                        try_again = False\n",
    "                    except Exception as err:\n",
    "                        log_print(\"Error: \" + str(err))\n",
    "                        log_print(\"Downloading image again: \" + image['url'])                  \n",
    "                        download_image(image['url'], imagePath)\n",
    "                        \n",
    "                        try_count += 1\n",
    "                        if try_count > 3:\n",
    "                            os.remove(imagePath)\n",
    "                            write_file(os.path.join(folder_path, \"deleted_images_path.txt\"), [imagePath, \"\\n\"])\n",
    "                            try_again = False\n",
    "\n",
    "            if item_copy[\"media_count\"] > 0:\n",
    "                posts_overview_clean[\"items\"].append(item_copy)\n",
    "                posts_overview_clean[\"num_results\"] += 1\n",
    "\n",
    "    if posts_overview_clean[\"num_results\"] > 0:\n",
    "        create_path(os.path.join(PATH_CLEAN_DATASET_TYPE, acc))\n",
    "        write_json(os.path.join(PATH_CLEAN_DATASET_TYPE, acc, \"user_posts_overview_clean.json\"), posts_overview_clean)\n",
    "\n",
    "    print(\"**********************************************\\n\")\n",
    "            \n",
    "log_print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef227b8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Moving Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb774498",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dbc4aa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list_accounts = get_accounts()\n",
    "\n",
    "for acc in list_accounts:\n",
    "    \n",
    "    log_print(f\"Processing account {acc}\")\n",
    "    \n",
    "    shutil.copyfile(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"user_info.json\"), os.path.join(PATH_CLEAN_DATASET_TYPE, acc, \"user_info.json\"))\n",
    "\n",
    "    create_path(os.path.join(PATH_CLEAN_DATASET_TYPE, acc, \"images\"))\n",
    "\n",
    "    overview_clean = read_json(os.path.join(PATH_CLEAN_DATASET_TYPE, acc, \"user_posts_overview_clean.json\"))\n",
    "\n",
    "    images_moved = len(list(next(os.walk(os.path.join(PATH_CLEAN_DATASET_TYPE, acc, \"images\")))[2])) - 1 #0 - root, 1 - dirs, 2 - files\n",
    "    if images_moved <= 0:\n",
    "        images_moved = 0\n",
    "    for item in overview_clean[\"items\"]:\n",
    "        for image in item[\"images\"]:\n",
    "            if not os.path.exists(os.path.join(PATH_CLEAN_DATASET_TYPE, acc, \"images\", str(image[\"pk\"]) + \".jpg\")):\n",
    "                shutil.move(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"images\", str(image[\"pk\"]) + \".jpg\"), os.path.join(PATH_CLEAN_DATASET_TYPE, acc, \"images\", str(image[\"pk\"]) + \".jpg\"))\n",
    "                images_moved += 1\n",
    "                log_print(f\"{images_moved}/{overview_clean['media_count_total']}\")\n",
    "    print(\"**********************************************\\n\")\n",
    "            \n",
    "log_print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b864ca3a",
   "metadata": {},
   "source": [
    "# Processing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ae884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a1a48c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a93e046",
   "metadata": {},
   "source": [
    "## Similary Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb25201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef7d4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users_comparison(i_followers, j_followers):\n",
    "    if i_followers >= j_followers:\n",
    "        diff_radius = i_followers * FOLLOWERS_RADIUS // 100\n",
    "    else:\n",
    "        diff_radius = j_followers * FOLLOWERS_RADIUS // 100\n",
    "\n",
    "    diff_abs = abs(i_followers - j_followers)\n",
    "    \n",
    "    return { \"comparison\": diff_abs <= diff_radius,\n",
    "           \"diff_abs\": diff_abs,\n",
    "           \"diff_radius\": diff_radius }\n",
    "    \n",
    "def get_items_comparison(i_likes, j_likes):\n",
    "    if i_likes >= j_likes:\n",
    "        diff_radius = i_likes * LIKES_RADIUS // 100\n",
    "    else:\n",
    "        diff_radius = j_likes * LIKES_RADIUS // 100        \n",
    "        \n",
    "    diff_abs = abs(i_likes - j_likes)\n",
    "    \n",
    "    if diff_abs >= diff_radius:\n",
    "        i_label = \"igual\"\n",
    "        j_label = \"igual\"\n",
    "\n",
    "        if i_likes > j_likes:\n",
    "            i_label = \"inte\"\n",
    "            j_label = \"desinte\"\n",
    "        elif i_likes < j_likes:\n",
    "            i_label = \"desinte\"\n",
    "            j_label = \"inte\"\n",
    "    else:\n",
    "        i_label = None\n",
    "        j_label = None\n",
    "    \n",
    "    return { \"comparison\": diff_abs >= diff_radius,\n",
    "           \"diff_abs\": diff_abs,\n",
    "           \"diff_radius\": diff_radius,\n",
    "           \"i_label\": i_label,\n",
    "           \"j_label\": j_label}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cde9e5",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_accounts = get_accounts()\n",
    "random.shuffle(list_accounts)\n",
    "tam = len(list_accounts)\n",
    "\n",
    "for i in range(tam-1):\n",
    "    \n",
    "    user_info_I = read_json(os.path.join(PATH_CLEAN_DATASET_TYPE, list_accounts[i], \"user_info.json\"))\n",
    "        \n",
    "    for j in range(i+1, tam):\n",
    "        comparisons_list = []\n",
    "\n",
    "        user_info_J = read_json(os.path.join(PATH_CLEAN_DATASET_TYPE, list_accounts[j], \"user_info.json\"))\n",
    "\n",
    "        log_print(f\"Comparando conta {i+1}-[{list_accounts[i]}] com conta {j+1}-[{list_accounts[j]}] / Total: {tam} contas.\")\n",
    "\n",
    "        users_comparison = get_users_comparison(user_info_I[\"follower_count\"], user_info_I[\"follower_count\"])\n",
    "        if users_comparison[\"comparison\"]:\n",
    "\n",
    "            user_posts_overview_clean_I = read_json(os.path.join(PATH_CLEAN_DATASET_TYPE, list_accounts[i], \"user_posts_overview_clean.json\"))\n",
    "            user_posts_overview_clean_J = read_json(os.path.join(PATH_CLEAN_DATASET_TYPE, list_accounts[j], \"user_posts_overview_clean.json\"))\n",
    "\n",
    "            random.shuffle(user_posts_overview_clean_I[\"items\"])\n",
    "            random.shuffle(user_posts_overview_clean_J[\"items\"])\n",
    "\n",
    "            for item_I in user_posts_overview_clean_I[\"items\"]:\n",
    "                for item_J in user_posts_overview_clean_J[\"items\"]:\n",
    "\n",
    "                    items_comparison = get_items_comparison(item_I[\"like_count\"], item_J[\"like_count\"])\n",
    "                    if (items_comparison[\"comparison\"]):\n",
    "\n",
    "                        row = {\n",
    "                             'type': TYPE,\n",
    "                             'i_username': list_accounts[i],\n",
    "                             'i_follower_count': user_info_I[\"follower_count\"],\n",
    "                             'j_username': list_accounts[j],\n",
    "                             'j_follower_count': user_info_J[\"follower_count\"],\n",
    "                             'diff_followers_abs': users_comparison[\"diff_abs\"],\n",
    "                             'diff_followers_radius': users_comparison[\"diff_radius\"],\n",
    "                             'i_pk': item_I[\"images\"][0][\"pk\"],\n",
    "                             'i_like_count': item_I[\"like_count\"],\n",
    "                             'j_pk': item_J[\"images\"][0][\"pk\"],\n",
    "                             'j_like_count': item_J[\"like_count\"],\n",
    "                             'diff_likes_abs': items_comparison[\"diff_abs\"],\n",
    "                             'diff_likes_radius': items_comparison[\"diff_radius\"],\n",
    "                             'i_label': items_comparison[\"i_label\"],\n",
    "                             'j_label': items_comparison[\"j_label\"]\n",
    "                        }\n",
    "\n",
    "                        comparisons_list.append(row)\n",
    "\n",
    "            if len(comparisons_list) > 0:\n",
    "                log_print(f\"Saving {len(comparisons_list)} new rows\")\n",
    "                df = pd.DataFrame(comparisons_list)\n",
    "                df.to_parquet(path=PATH_SIMILARY_ACCOUNTS_TYPE, partition_cols=\"i_username\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac35586",
   "metadata": {},
   "source": [
    "## Label Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e18714",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similary_accounts = pd.read_parquet(path=PATH_SIMILARY_ACCOUNTS_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb3f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = df_similary_accounts.loc[:, [\"type\", \"i_username\", \"i_follower_count\", \"i_pk\", \"i_like_count\", \"i_label\"]].copy()\n",
    "df_j = df_similary_accounts.loc[:, [\"type\", \"j_username\", \"j_follower_count\", \"j_pk\", \"j_like_count\", \"j_label\"]].copy()\n",
    "\n",
    "columns = [\"type\", \"username\", \"follower_count\", \"pk\", \"like_count\", \"label\"]\n",
    "df_i.columns = columns\n",
    "df_j.columns = columns\n",
    "\n",
    "df_united = pd.concat([df_i, df_j], axis=\"index\")\n",
    "\n",
    "df_label_counter_aux = df_united.groupby(columns).size().reset_index(name=\"count\").sort_values([\"pk\",\"count\"]).drop_duplicates(subset=['pk'], keep='first').copy()\n",
    "df_label_counter_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a515c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RANDOM_IGUAL:\n",
    "    df_label_counter = df_label_counter_aux.copy()\n",
    "    df_label_counter.loc[df_label_counter['label'] == 'igual', 'label'] = [ random.choice([\"inte\", \"desinte\"]) for k in df_label_counter.loc[df_label_counter['label'] == 'igual'].index ]\n",
    "else:\n",
    "    df_label_counter = df_label_counter.loc[df_label_counter['label'] != 'igual'].copy()\n",
    "df_label_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75afc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_counter.to_parquet(path=PATH_LABEL_COUNTER_TYPE, partition_cols=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbed0da",
   "metadata": {},
   "source": [
    "# Classifying Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2843abc7",
   "metadata": {},
   "source": [
    "## CNN Classify Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38bc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_CNN_CLASSIFY_BASE                       = os.path.join(PATH_4_CLASSIFYING_DATASET, \"CNN_Classify\")\n",
    "PATH_CNN_CLASSIFY_OUTPUT                     = os.path.join(PATH_CNN_CLASSIFY_BASE, \"CNN_Output\")\n",
    "PATH_CNN_CLASSIFY_OUTPUT_TYPE                = os.path.join(PATH_CNN_CLASSIFY_OUTPUT, TYPE)\n",
    "\n",
    "print(f\"PATH_CNN_CLASSIFY_BASE                      : {PATH_CNN_CLASSIFY_BASE}\")\n",
    "print(f\"PATH_CNN_CLASSIFY_OUTPUT                    : {PATH_CNN_CLASSIFY_OUTPUT}\")\n",
    "print(f\"PATH_CNN_CLASSIFY_OUTPUT_TYPE               : {PATH_CNN_CLASSIFY_OUTPUT_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f9c0d1",
   "metadata": {},
   "source": [
    "### Reset Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de61f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_path(PATH_CNN_CLASSIFY_OUTPUT_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce67173",
   "metadata": {},
   "source": [
    "### Checking Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d56b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_path(PATH_CNN_CLASSIFY_BASE)\n",
    "#create_path(PATH_CNN_CLASSIFY_OUTPUT)\n",
    "create_path(PATH_CNN_CLASSIFY_OUTPUT_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f27a32",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec02f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_counter = pd.read_parquet(path=PATH_LABEL_COUNTER_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bcc4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_interessantes = len(df_label_counter.loc[df_label_counter['label'] == 'inte'].index)\n",
    "count_desinteressantes = len(df_label_counter.loc[df_label_counter['label'] == 'desinte'].index)\n",
    "\n",
    "count_part_interessantes = count_interessantes // CV\n",
    "count_part_desinteressantes = count_desinteressantes // CV\n",
    "count_total = (count_part_interessantes + count_part_desinteressantes) * CV\n",
    "\n",
    "print(f\"Type: {TYPE}\")\n",
    "print(f\"Interessantes: {count_part_interessantes * CV}\")\n",
    "print(f\"Desinteressantes: {count_part_desinteressantes * CV}\")\n",
    "print(f\"Total: {count_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4708a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interessantes = df_label_counter.loc[df_label_counter['label'] == 'inte'].sample(n = count_interessantes).copy()\n",
    "df_desinteressantes = df_label_counter.loc[df_label_counter['label'] == 'desinte'].sample(n = count_desinteressantes).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d30f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.concat([df_interessantes, df_desinteressantes], axis=\"index\")\n",
    "df_data = df_data.sample(frac=1).reset_index(drop=True)\n",
    "df_data['imagePath'] = df_data.apply(lambda row: os.path.join(PATH_CLEAN_DATASET_TYPE, row.username, 'images', str(row.pk) + '.jpg'), axis=1)\n",
    "\n",
    "imagePaths = df_data.loc[:, 'imagePath'].copy().values.tolist()\n",
    "labels = df_data.loc[:, 'label'].copy().values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4c11a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit([\"desinte\", \"inte\"])\n",
    "# serialize the label encoder to disk\n",
    "\n",
    "joblib.dump(le, os.path.join(PATH_CNN_CLASSIFY_OUTPUT_TYPE, \"le.pickle\"))\n",
    "\n",
    "print(\"[INFO] loading network...\")\n",
    "model = VGG16(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "print(\"[INFO] processing...\")\n",
    "# open the output CSV file for writing\n",
    "csv = open(os.path.join(PATH_CNN_CLASSIFY_OUTPUT_TYPE, \"data.csv\"), \"w\")\n",
    "\n",
    "loop_images_extract_features(imagePaths, labels, csv)\n",
    "\n",
    "# close the CSV file\n",
    "csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0bfbc5",
   "metadata": {},
   "source": [
    "## Train With Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e672b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive the paths to the training and testing CSV files\n",
    "dataPath = os.path.join(PATH_CNN_CLASSIFY_OUTPUT_TYPE, \"data.csv\")\n",
    "\n",
    "# load the data from disk\n",
    "print(\"[INFO] loading data...\")\n",
    "(X, y) = load_data_split(dataPath)\n",
    "# load the label encoder from disk\n",
    "le = joblib.load(os.path.join(PATH_CNN_CLASSIFY_OUTPUT_TYPE, \"le.pickle\"))\n",
    "\n",
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "predicted = model_selection.cross_val_predict(LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\"), X, y, cv=CV)\n",
    "\n",
    "# evaluate the model\n",
    "print(\"[INFO] evaluating...\")\n",
    "plot_confusion_matrix(y, predicted, np.array(le.classes_), normalize=True)\n",
    "#plt.show()\n",
    "result = metrics.classification_report(y, predicted, target_names=le.classes_)\n",
    "print(result)\n",
    "\n",
    "# save the result to disk\n",
    "print(\"[INFO] saving result...\")\n",
    "plt.savefig(os.path.join(PATH_CNN_CLASSIFY_OUTPUT_TYPE, \"test_normalized.svg\"))\n",
    "write_file(os.path.join(PATH_CNN_CLASSIFY_OUTPUT_TYPE, \"result.txt\"), [result, \"\\n\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d97636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "notify_time": "10",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "308px",
    "width": "254px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "346px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "815px",
    "left": "1557px",
    "right": "20px",
    "top": "115px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
