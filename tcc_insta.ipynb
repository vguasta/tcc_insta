{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c216a87c",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bea1de",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24220d08",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#On Anaconda Prompt:\n",
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69866f0b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e27de2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pip install requests_toolbelt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba638d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0069b5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37835719",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02279c82",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27732f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507a16c4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb981b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pip install pandavro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d7fe35",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3563d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaffedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.InstagramAPI.InstagramAPI import InstagramAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff01e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths #pip install opencv-python\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5d6962",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30f66ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE_BEGIN                       : 1514775600.0\n",
      "PATH_BASE                        : D:\\IC\\tcc_insta\n",
      "PATH_BASE_DATA                   : D:\\IC\\tcc_insta\\data\n",
      "PATH_1_CREATING_DATASET          : D:\\IC\\tcc_insta\\data\\1-Creating_Dataset\n",
      "PATH_FIRST_DATASET               : D:\\IC\\tcc_insta\\data\\1-Creating_Dataset\\Dataset\n",
      "PATH_FIRST_DATASET_TYPE          : D:\\IC\\tcc_insta\\data\\1-Creating_Dataset\\Dataset\\travel\n",
      "PATH_FIRST_STATISTICS            : D:\\IC\\tcc_insta\\data\\1-Creating_Dataset\\Statistics\n",
      "PATH_FIRST_STATISTICS_TYPE       : D:\\IC\\tcc_insta\\data\\1-Creating_Dataset\\Statistics\\travel\n",
      "PATH_2_PREPARING_DATASET         : D:\\IC\\tcc_insta\\data\\2-Preparing_Dataset\n",
      "PATH_CLEAN_DATASET               : D:\\IC\\tcc_insta\\data\\2-Preparing_Dataset\\Clean_Dataset\n",
      "PATH_CLEAN_DATASET_TYPE          : D:\\IC\\tcc_insta\\data\\2-Preparing_Dataset\\Clean_Dataset\\travel\n",
      "PATH_CLEAN_STATISTICS            : D:\\IC\\tcc_insta\\data\\2-Preparing_Dataset\\Statistics\n",
      "PATH_CLEAN_STATISTICS_TYPE       : D:\\IC\\tcc_insta\\data\\2-Preparing_Dataset\\Statistics\\travel\n",
      "PATH_3_PROCESSING_DATASET        : D:\\IC\\tcc_insta\\data\\3-Processing_Dataset\n",
      "PATH_SIMILARY_ACCOUNTS           : D:\\IC\\tcc_insta\\data\\3-Processing_Dataset\\Similary_Accounts\n",
      "PATH_SIMILARY_ACCOUNTS_TYPE      : D:\\IC\\tcc_insta\\data\\3-Processing_Dataset\\Similary_Accounts\\travel\n",
      "PATH_LABEL_COUNTER               : D:\\IC\\tcc_insta\\data\\3-Processing_Dataset\\Label_Counter\n",
      "PATH_LABEL_COUNTER_TYPE          : D:\\IC\\tcc_insta\\data\\3-Processing_Dataset\\Label_Counter\\travel\n",
      "PATH_4_CLASSIFYING_DATASET       : D:\\IC\\tcc_insta\\data\\4-Classifying_Dataset\n",
      "PATH_FINAL_DATASET               : D:\\IC\\tcc_insta\\data\\4-Classifying_Dataset\\Dataset\n",
      "PATH_FINAL_DATASET_TYPE          : D:\\IC\\tcc_insta\\data\\4-Classifying_Dataset\\Dataset\\travel\n",
      "PATH_FINAL_DATASET_TYPE_TRAIN    : D:\\IC\\tcc_insta\\data\\4-Classifying_Dataset\\Dataset\\travel\\2_classes\\Train\n",
      "PATH_FINAL_DATASET_TYPE_VALIDATE : D:\\IC\\tcc_insta\\data\\4-Classifying_Dataset\\Dataset\\travel\\2_classes\\Validate\n",
      "PATH_FINAL_DATASET_TYPE_TEST     : D:\\IC\\tcc_insta\\data\\4-Classifying_Dataset\\Dataset\\travel\\2_classes\\Test\n"
     ]
    }
   ],
   "source": [
    "TYPE = \"travel\"\n",
    "\n",
    "DATE_BEGIN = datetime(2018,1,1,0,0).timestamp()\n",
    "\n",
    "FINAL_CLASSES = [\"desinte\", \"inte\"]\n",
    "#FINAL_CLASSES = [\"m.desinte\", \"desinte\", \"inte\", \"m.inte\"]\n",
    "LEN_FINAL_CLASSES = len(FINAL_CLASSES)\n",
    "\n",
    "PATH_BASE                        = os.getcwd()\n",
    "PATH_BASE_DATA                   = os.path.join(PATH_BASE, \"data\")\n",
    "PATH_1_CREATING_DATASET          = os.path.join(PATH_BASE_DATA, \"1-Creating_Dataset\")\n",
    "PATH_FIRST_DATASET               = os.path.join(PATH_1_CREATING_DATASET, \"Dataset\")\n",
    "PATH_FIRST_DATASET_TYPE          = os.path.join(PATH_FIRST_DATASET, TYPE)\n",
    "PATH_FIRST_STATISTICS            = os.path.join(PATH_1_CREATING_DATASET, \"Statistics\")\n",
    "PATH_FIRST_STATISTICS_TYPE       = os.path.join(PATH_FIRST_STATISTICS, TYPE)\n",
    "PATH_2_PREPARING_DATASET         = os.path.join(PATH_BASE_DATA, \"2-Preparing_Dataset\")\n",
    "PATH_CLEAN_DATASET               = os.path.join(PATH_2_PREPARING_DATASET, \"Clean_Dataset\")\n",
    "PATH_CLEAN_DATASET_TYPE          = os.path.join(PATH_CLEAN_DATASET, TYPE)\n",
    "PATH_CLEAN_STATISTICS            = os.path.join(PATH_2_PREPARING_DATASET, \"Statistics\")\n",
    "PATH_CLEAN_STATISTICS_TYPE       = os.path.join(PATH_CLEAN_STATISTICS, TYPE)\n",
    "PATH_3_PROCESSING_DATASET        = os.path.join(PATH_BASE_DATA, \"3-Processing_Dataset\")\n",
    "PATH_SIMILARY_ACCOUNTS           = os.path.join(PATH_3_PROCESSING_DATASET, \"Similary_Accounts\")\n",
    "PATH_SIMILARY_ACCOUNTS_TYPE      = os.path.join(PATH_SIMILARY_ACCOUNTS, TYPE)\n",
    "PATH_LABEL_COUNTER               = os.path.join(PATH_3_PROCESSING_DATASET, \"Label_Counter\")\n",
    "PATH_LABEL_COUNTER_TYPE          = os.path.join(PATH_LABEL_COUNTER, TYPE)\n",
    "PATH_4_CLASSIFYING_DATASET       = os.path.join(PATH_BASE_DATA, \"4-Classifying_Dataset\")\n",
    "PATH_FINAL_DATASET               = os.path.join(PATH_4_CLASSIFYING_DATASET, \"Dataset\")\n",
    "PATH_FINAL_DATASET_TYPE          = os.path.join(PATH_FINAL_DATASET, TYPE)\n",
    "PATH_FINAL_DATASET_TYPE_TRAIN    = os.path.join(PATH_FINAL_DATASET_TYPE, f\"{LEN_FINAL_CLASSES}_classes\", \"Train\")\n",
    "PATH_FINAL_DATASET_TYPE_VALIDATE = os.path.join(PATH_FINAL_DATASET_TYPE, f\"{LEN_FINAL_CLASSES}_classes\", \"Validate\")\n",
    "PATH_FINAL_DATASET_TYPE_TEST     = os.path.join(PATH_FINAL_DATASET_TYPE, f\"{LEN_FINAL_CLASSES}_classes\", \"Test\")\n",
    "\n",
    "MIN_FOLLOWERS = 1000\n",
    "MEDIA_TYPES = [1] #8\n",
    "CANDIDATE = \"N\"\n",
    "#CANDIDATE = -1\n",
    "#CANDIDATE = 0\n",
    "#CANDIDATE = 1\n",
    "#CANDIDATE = 2\n",
    "\n",
    "ONLY_WITH_COMMENTS = True\n",
    "\n",
    "#CNN_MODEL_CLEAN = \"VGG16\"\n",
    "#CNN_MODEL_CLEAN = \"RESNET\"\n",
    "CNN_MODEL_CLEAN = \"INCEPTIONV3\"\n",
    "\n",
    "#CLASSIFIER_CLEAN = \"KNN\"\n",
    "#CLASSIFIER_CLEAN = \"NB\"\n",
    "CLASSIFIER_CLEAN = \"RANDOMFOREST\"\n",
    "\n",
    "PRINT_IMAGE_AND_PREDICTED_LABELS = False\n",
    "DISPLAY_IMAGE_LABEL = f\"non_{TYPE}\"\n",
    "\n",
    "FOLLOWERS_RADIUS = 5\n",
    "LIKES_RADIUS = 95\n",
    "TIMESTAMP_DIFFERENCE = 30 * 86400\n",
    "\n",
    "RANDOMIZE_IF_EQUALS = True\n",
    "#RANDOMIZE_IF_EQUALS = False\n",
    "\n",
    "BATCH_SIZE = 24\n",
    "\n",
    "CNN_MODEL_FINAL = \"VGG16\"\n",
    "#CNN_MODEL_FINAL = \"RESNET\"\n",
    "#CNN_MODEL_FINAL = \"INCEPTIONV3\"\n",
    "\n",
    "#CLASSIFIER_FINAL = \"KNN\"\n",
    "#CLASSIFIER_FINAL = \"NB\"\n",
    "#CLASSIFIER_FINAL = \"RANDOMFOREST\"\n",
    "CLASSIFIER_FINAL = \"CNN\"\n",
    "\n",
    "EPOCHS = 20\n",
    "CV = 5\n",
    "\n",
    "print(f\"DATE_BEGIN                       : {DATE_BEGIN}\")\n",
    "print(f\"PATH_BASE                        : {PATH_BASE}\")\n",
    "print(f\"PATH_BASE_DATA                   : {PATH_BASE_DATA}\")\n",
    "print(f\"PATH_1_CREATING_DATASET          : {PATH_1_CREATING_DATASET}\")\n",
    "print(f\"PATH_FIRST_DATASET               : {PATH_FIRST_DATASET}\")\n",
    "print(f\"PATH_FIRST_DATASET_TYPE          : {PATH_FIRST_DATASET_TYPE}\")\n",
    "print(f\"PATH_FIRST_STATISTICS            : {PATH_FIRST_STATISTICS}\")\n",
    "print(f\"PATH_FIRST_STATISTICS_TYPE       : {PATH_FIRST_STATISTICS_TYPE}\")\n",
    "print(f\"PATH_2_PREPARING_DATASET         : {PATH_2_PREPARING_DATASET}\")\n",
    "print(f\"PATH_CLEAN_DATASET               : {PATH_CLEAN_DATASET}\")\n",
    "print(f\"PATH_CLEAN_DATASET_TYPE          : {PATH_CLEAN_DATASET_TYPE}\")\n",
    "print(f\"PATH_CLEAN_STATISTICS            : {PATH_CLEAN_STATISTICS}\")\n",
    "print(f\"PATH_CLEAN_STATISTICS_TYPE       : {PATH_CLEAN_STATISTICS_TYPE}\")\n",
    "print(f\"PATH_3_PROCESSING_DATASET        : {PATH_3_PROCESSING_DATASET}\")\n",
    "print(f\"PATH_SIMILARY_ACCOUNTS           : {PATH_SIMILARY_ACCOUNTS}\")\n",
    "print(f\"PATH_SIMILARY_ACCOUNTS_TYPE      : {PATH_SIMILARY_ACCOUNTS_TYPE}\")\n",
    "print(f\"PATH_LABEL_COUNTER               : {PATH_LABEL_COUNTER}\")\n",
    "print(f\"PATH_LABEL_COUNTER_TYPE          : {PATH_LABEL_COUNTER_TYPE}\")\n",
    "print(f\"PATH_4_CLASSIFYING_DATASET       : {PATH_4_CLASSIFYING_DATASET}\")\n",
    "print(f\"PATH_FINAL_DATASET               : {PATH_FINAL_DATASET}\")\n",
    "print(f\"PATH_FINAL_DATASET_TYPE          : {PATH_FINAL_DATASET_TYPE}\")\n",
    "print(f\"PATH_FINAL_DATASET_TYPE_TRAIN    : {PATH_FINAL_DATASET_TYPE_TRAIN}\")\n",
    "print(f\"PATH_FINAL_DATASET_TYPE_VALIDATE : {PATH_FINAL_DATASET_TYPE_VALIDATE}\")\n",
    "print(f\"PATH_FINAL_DATASET_TYPE_TEST     : {PATH_FINAL_DATASET_TYPE_TEST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a68d5",
   "metadata": {},
   "source": [
    "## Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07557f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK.\n"
     ]
    }
   ],
   "source": [
    "def write_file(path_file, data, access_mode='a'):\n",
    "    file = open(path_file, access_mode)\n",
    "    if isinstance(data, list):\n",
    "        for d in data:\n",
    "            file.write(d)\n",
    "    else:\n",
    "        file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "def read_file(path_file, access_mode='r'):\n",
    "    file = open(path_file, access_mode)\n",
    "    data = file.readlines()\n",
    "    file.close()  \n",
    "    return data\n",
    "\n",
    "def write_json(json_path, data, access_mode='w', encoding=\"utf-8\"):\n",
    "    with open(json_path, access_mode, encoding=encoding) as json_file:\n",
    "        json.dump(data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "def read_json(json_path, access_mode='r', encoding=\"utf-8\"):\n",
    "    with open(json_path, access_mode, encoding=encoding) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "##############################################\n",
    "\n",
    "def get_accounts():\n",
    "    list_accounts_file = read_file(os.path.join(PATH_1_CREATING_DATASET, f\"{TYPE}_users.txt\"))\n",
    "    list_accounts = []\n",
    "    for acc_file in list_accounts_file:\n",
    "        acc = acc_file.strip().lower()\n",
    "        if acc[0] != \"#\":\n",
    "            list_accounts.append(acc)\n",
    "    return list_accounts\n",
    "\n",
    "def log_print(data):\n",
    "    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {data}\", flush=True)\n",
    "\n",
    "def create_path(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        log_print(f\"Path '{path}' was created.\")\n",
    "\n",
    "def delete_path(path):\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path, ignore_errors=True)\n",
    "        log_print(f\"Path '{path}' was deleted.\")\n",
    "\n",
    "def random_sleep(min=0.1, max=3):\n",
    "    time.sleep(round(random.uniform(min, max), 1))\n",
    "\n",
    "##############################################\n",
    "\n",
    "def remove_users_without_images(dataset_path, dataset_folder=\"Dataset\", overview_file=\"user_posts_overview.json\"):\n",
    "    log_print(f\"Beginning...\")\n",
    "    type_folder = os.path.join(dataset_path, dataset_folder, TYPE)\n",
    "    for user_folder in next(os.walk(type_folder))[1]: #0 - root, 1 - dirs, 2 - files\n",
    "        user_path = os.path.join(type_folder, user_folder)\n",
    "        user_images_path = os.path.join(user_path, \"images\")\n",
    "        try:\n",
    "            posts_overview = read_json(os.path.join(user_path, overview_file))\n",
    "            if (len(next(os.walk(user_images_path))[2]) == 0) or (\"media_count_total\" in posts_overview and posts_overview[\"media_count_total\"] == 0) or (\"image_count_total\" in posts_overview and posts_overview[\"image_count_total\"] == 0):\n",
    "                log_print(user_folder)\n",
    "                write_file(os.path.join(dataset_path, f\"{TYPE}_users_error.log\"), [f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - RemoveUsersWithoutImages - {user_folder}\", \"\\n\"])\n",
    "        except Exception as e:\n",
    "            log_print(f\"ERROR: {user_folder} - {e}\")\n",
    "            write_file(os.path.join(dataset_path, f\"{TYPE}_users_error.log\"), [f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - RemoveUsersWithoutImages - {user_folder} - {e}\", \"\\n\"])\n",
    "    log_print(f\"Finished\")  \n",
    "        \n",
    "def create_histogram(array, xlabel, ylabel, title, dataset_path, file_name, bins='auto', figsize=(7,6)):\n",
    "    log_print(f\"Creating {title} histogram...\")\n",
    "    h = plt.figure(figsize=figsize)\n",
    "    n, bins, patches = plt.hist(array, bins=bins, edgecolor='black', linewidth=1)\n",
    "    rotation = 65\n",
    "    for i in range(len(bins)-1):\n",
    "        if n[i] > 0:\n",
    "            plt.text(bins[i]+((bins[i+1]-bins[i])/3), n[i]+(n[i]*0.02), str(math.ceil(n[i])), rotation=rotation)\n",
    "    plt.grid(True)\n",
    "    plt.xticks(bins, rotation=rotation)\n",
    "    plt.style.use('seaborn-white')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.savefig(os.path.join(dataset_path, \"Statistics\", TYPE, file_name))\n",
    "    log_print(\"Histogram created!\")\n",
    "            \n",
    "def generate_statistics(dataset_path, dataset_folder=\"Dataset\", overview_file=\"user_posts_overview.json\"):\n",
    "    list_accounts = get_accounts()\n",
    "    df_statistics = pd.DataFrame()\n",
    "    #total_count_media = 0\n",
    "    log_print(\"Getting statistics...\")\n",
    "    \n",
    "    array_likes = []\n",
    "    array_comments = []\n",
    "    array_likes_comments = []\n",
    "    \n",
    "    for acc in list_accounts:\n",
    "        log_print(f\"Executing account {acc}\")\n",
    "        user_file = read_json(os.path.join(dataset_path, dataset_folder, TYPE, acc, \"user_info.json\"))\n",
    "        posts_overview = read_json(os.path.join(dataset_path, dataset_folder, TYPE, acc, overview_file))\n",
    "        #total_count_media += posts_overview[\"media_count_total\"]\n",
    "        \n",
    "        min_likes = posts_overview[\"items\"][0][\"like_count\"]\n",
    "        max_likes = posts_overview[\"items\"][0][\"like_count\"]\n",
    "        min_comments = posts_overview[\"items\"][0][\"comment_count\"]\n",
    "        max_comments = posts_overview[\"items\"][0][\"comment_count\"]\n",
    "        min_likes_comments = posts_overview[\"items\"][0][\"like_count\"]\n",
    "        max_likes_comments = posts_overview[\"items\"][0][\"like_count\"]\n",
    "        \n",
    "        pk_min_likes = posts_overview[\"items\"][0][\"pk\"]\n",
    "        pk_max_likes = posts_overview[\"items\"][0][\"pk\"]\n",
    "        pk_min_comments = posts_overview[\"items\"][0][\"pk\"]\n",
    "        pk_max_comments = posts_overview[\"items\"][0][\"pk\"]\n",
    "        pk_min_likes_comments = posts_overview[\"items\"][0][\"pk\"]\n",
    "        pk_max_likes_comments = posts_overview[\"items\"][0][\"pk\"]\n",
    "        \n",
    "        for item in posts_overview[\"items\"]:\n",
    "            \n",
    "            array_likes.append(item[\"like_count\"])\n",
    "            array_comments.append(item[\"comment_count\"])\n",
    "            array_likes_comments.append(item[\"like_count\"] + item[\"comment_count\"])\n",
    "            \n",
    "            if item[\"like_count\"] < min_likes:\n",
    "                pk_min_likes = item[\"pk\"]\n",
    "                min_likes = item[\"like_count\"]\n",
    "            \n",
    "            if item[\"like_count\"] > max_likes:\n",
    "                pk_max_likes = item[\"pk\"]\n",
    "                max_likes = item[\"like_count\"]\n",
    "            \n",
    "            if item[\"comment_count\"] < min_comments:\n",
    "                pk_min_comments = item[\"pk\"]\n",
    "                min_comments = item[\"comment_count\"]\n",
    "            \n",
    "            if item[\"comment_count\"] > max_comments:\n",
    "                pk_max_comments = item[\"pk\"]\n",
    "                max_comments = item[\"comment_count\"]\n",
    "                \n",
    "            if item[\"like_count\"] + item[\"comment_count\"] < min_likes_comments:\n",
    "                pk_min_likes_comments = item[\"pk\"]\n",
    "                min_likes_comments = item[\"like_count\"] + item[\"comment_count\"]\n",
    "            \n",
    "            if item[\"like_count\"] + item[\"comment_count\"] > max_likes_comments:\n",
    "                pk_max_likes_comments = item[\"pk\"]\n",
    "                max_likes_comments = item[\"like_count\"] + item[\"comment_count\"]\n",
    "        \n",
    "        if \"media_count_total\" in posts_overview:\n",
    "            media_count_total = int(math.ceil(posts_overview[\"media_count_total\"]))\n",
    "        else:\n",
    "            media_count_total = int(math.ceil(posts_overview[\"image_count_total\"]))\n",
    "        \n",
    "        new_row = {\n",
    "            'user': acc,\n",
    "            'followers_count': user_file[\"follower_count\"],\n",
    "            'media_count_total': media_count_total,\n",
    "            'min_likes': int(math.ceil(min_likes)), \n",
    "            'max_likes': math.ceil(max_likes),\n",
    "            'min_comments': math.ceil(min_comments),\n",
    "            'max_comments': math.ceil(max_comments),\n",
    "            'min_likes_comments': math.ceil(min_likes_comments),\n",
    "            'max_likes_comments': math.ceil(max_likes_comments),\n",
    "            'pk_min_likes': f\"'{pk_min_likes}'\",\n",
    "            'pk_max_likes': f\"'{pk_max_likes}'\",\n",
    "            'pk_min_comments': f\"'{pk_min_comments}'\",\n",
    "            'pk_max_comments': f\"'{pk_max_comments}'\",\n",
    "            'pk_min_likes_comments': f\"'{pk_min_likes_comments}'\",\n",
    "            'pk_max_likes_comments': f\"'{pk_max_likes_comments}'\"\n",
    "        }\n",
    "        df_statistics = df_statistics.append(new_row, ignore_index=True)\n",
    "    \n",
    "    df_statistics = df_statistics.sort_values(by=['followers_count'], ascending=False)\n",
    "    \n",
    "    log_print(\"Finished\")\n",
    "    \n",
    "    return {\n",
    "        \"df_statistics\": df_statistics,\n",
    "        \"array_likes\": array_likes,\n",
    "        \"array_comments\": array_comments,\n",
    "        \"array_likes_comments\": array_likes_comments\n",
    "    }\n",
    "    \n",
    "def remove_outliers(df, col):\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1 #IQR is interquartile range. \n",
    "\n",
    "    filter = (df[col] >= Q1 - 1.5 * IQR) & (df[col] <= Q3 + 1.5 *IQR)\n",
    "    return df.loc[filter]\n",
    "\n",
    "def generate_boxplots(df_statistics, dataset_path):\n",
    "    file_name = \"statistics_with_outliers.csv\"\n",
    "    log_print(f\"Creating file {file_name}\")\n",
    "    df_statistics_with_outliers = df_statistics.copy()\n",
    "    df_statistics_with_outliers.to_csv(os.path.join(dataset_path, \"Statistics\", TYPE, file_name), index=False)\n",
    "    log_print(f\"File {file_name} created!\")\n",
    "\n",
    "    file_name = \"boxplot_with_outliers.png\"\n",
    "    log_print(f\"Creating file {file_name}\")\n",
    "    plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]\n",
    "    plt.clf()\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    ax = sns.boxplot(x=df_statistics[\"followers_count\"])\n",
    "    ax.get_figure().savefig(os.path.join(dataset_path, \"Statistics\", TYPE, file_name))\n",
    "    log_print(f\"File {file_name} created!\")\n",
    "    \n",
    "    file_name = \"boxplot_without_outliers.png\"\n",
    "    log_print(f\"Creating file {file_name}\")\n",
    "    plt.clf()\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    ax = sns.boxplot(x=df_statistics[\"followers_count\"], showfliers=False)\n",
    "    ax.get_figure().savefig(os.path.join(dataset_path, \"Statistics\", TYPE, file_name))\n",
    "    log_print(f\"File {file_name} created!\")\n",
    "    \n",
    "    log_print(\"Removing outliers...\")\n",
    "    df_statistics = remove_outliers(df_statistics, \"followers_count\")\n",
    "    df_statistics = df_statistics.sort_values(by=['followers_count'], ascending=False)\n",
    "    \n",
    "    file_name = \"statistics_without_outliers.csv\"\n",
    "    log_print(f\"Creating file {file_name}\")\n",
    "    df_statistics_without_outliers = df_statistics.copy()\n",
    "    df_statistics_without_outliers.to_csv(os.path.join(dataset_path, \"Statistics\", TYPE, file_name), index=False)\n",
    "    log_print(f\"File {file_name} created!\")\n",
    "    \n",
    "    return df_statistics\n",
    "    \n",
    "##############################################\n",
    "\n",
    "def get_cnn_model(cnn_model_type):\n",
    "    \n",
    "    log_print(f\"CNN_MODEL: {cnn_model_type}\")\n",
    "    log_print(\"Loading network...\")\n",
    "    \n",
    "    if cnn_model_type == \"VGG16\":\n",
    "        cnn_model = tf.keras.applications.vgg16.VGG16(weights=\"imagenet\", include_top=False)\n",
    "        target_size = (224, 224)\n",
    "        final_shape = 7*7*512\n",
    "        preprocess_func = tf.keras.applications.vgg16.preprocess_input\n",
    "    elif cnn_model_type == \"RESNET\":\n",
    "        cnn_model = tf.keras.applications.resnet.ResNet101(weights=\"imagenet\", include_top=False)\n",
    "        target_size = (224, 224)\n",
    "        final_shape = 7*7*2048\n",
    "        preprocess_func = tf.keras.applications.resnet.preprocess_input\n",
    "    elif cnn_model_type == \"INCEPTIONV3\":\n",
    "        cnn_model = tf.keras.applications.InceptionV3(weights=\"imagenet\", include_top=False)\n",
    "        target_size = (299, 299)\n",
    "        final_shape = 8*8*2048\n",
    "        preprocess_func = tf.keras.applications.inception_v3.preprocess_input\n",
    "    else:\n",
    "        log_print(f\"INVALID CNN_MODEL: {cnn_model_type}\")\n",
    "        return None, (0,0), 0\n",
    "    \n",
    "    log_print(\"Network loaded.\")\n",
    "    return cnn_model, target_size, final_shape, preprocess_func\n",
    "    \n",
    "def get_classifier_model(classifier_model_type):\n",
    "    \n",
    "    log_print(f\"CLASSIFIER: {classifier_model_type}\")\n",
    "    log_print(\"Loading model...\")\n",
    "    \n",
    "    if classifier_model_type == \"KNN\":\n",
    "        classifier_model = KNeighborsClassifier(n_neighbors=3)\n",
    "    elif classifier_model_type == \"NB\":\n",
    "        classifier_model = GaussianNB()\n",
    "    elif classifier_model_type == \"RANDOMFOREST\":\n",
    "        classifier_model = RandomForestClassifier()\n",
    "    else:\n",
    "        log_print(f\"INVALID CLASSIFIER: {classifier_model_type}\")\n",
    "        return None\n",
    "        \n",
    "    log_print(\"Model loaded.\")\n",
    "    return classifier_model\n",
    "\n",
    "def load_image_from_path(imagePath, target_size=(224, 224)):\n",
    "    # load the input image using the Keras helper utility while ensuring the image is resized to 224x224 pixels\n",
    "    image = load_img(imagePath, target_size=target_size)\n",
    "    image = img_to_array(image)\n",
    "\n",
    "    # preprocess the image by (1) expanding the dimensions and\n",
    "    # (2) subtracting the mean RGB pixel intensity from the ImageNet dataset\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = imagenet_utils.preprocess_input(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "def loop_images_extract_features(model, target_size, final_shape, df_images, columns_names_with_labels, output_path=\"\"):\n",
    "    \n",
    "    len_df_images = len(df_images)\n",
    "    \n",
    "    log_print(\"Loading all images...\")\n",
    "    df_images['image'] = df_images.apply(lambda row: load_image_from_path(row.imagePath, target_size), axis=1)\n",
    "\n",
    "    log_print(\"Predicting...\")\n",
    "    features = model.predict(np.vstack(df_images.loc[:, \"image\"].copy().to_list()), batch_size=BATCH_SIZE, use_multiprocessing=True)      \n",
    "    log_print(features.shape)\n",
    "    features = features.reshape((features.shape[0], final_shape))\n",
    "\n",
    "    df_images = df_images.drop(columns=[\"image\", \"imagePath\"])\n",
    "    \n",
    "    log_print(\"Concatenating...\")\n",
    "    df_images = pd.concat([df_images, pd.DataFrame(features)], axis=1)\n",
    "\n",
    "    log_print(\"Converting columns type...\")\n",
    "    df_images.columns = df_images.columns.astype(str)\n",
    "    \n",
    "    return df_images\n",
    "    #log_print(\"Saving...\")\n",
    "    #pdx.to_avro(output_path, df_images)\n",
    "\n",
    "\n",
    "##################################################\n",
    "\n",
    "def error_log(acc, log_type, e):\n",
    "    log_print(f\"Failed to process {acc} account: {str(e)}\")\n",
    "    log_print(f\"Appending to file {TYPE}_users_error.log\")\n",
    "    write_file(os.path.join(PATH_1_CREATING_DATASET, f\"{TYPE}_users_error.log\"), \\\n",
    "               [f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {log_type} - {acc} - {str(e)}\", \"\\n\"])\n",
    "    print(\"**********************************************\\n\")\n",
    "\n",
    "##################################################\n",
    "\n",
    "def download_image(url, path):\n",
    "    resp = requests.get(url, stream=True, timeout=20)  # Open the url image, set stream to True, this will return the stream content.\n",
    "    local_file = open(path, 'wb')  # Open a local file with wb ( write binary ) permission.\n",
    "    resp.raw.decode_content = True  # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "    shutil.copyfileobj(resp.raw, local_file)  # Copy the response stream raw data to local image file.\n",
    "\n",
    "print(\"OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c0dc79",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Reset Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243eadbe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "delete_path(PATH_FIRST_DATASET_TYPE)\n",
    "delete_path(PATH_FIRST_STATISTICS_TYPE)\n",
    "delete_path(PATH_CLEAN_DATASET_TYPE)\n",
    "delete_path(PATH_SIMILARY_ACCOUNTS_TYPE)\n",
    "delete_path(PATH_LABEL_COUNTER_TYPE)\n",
    "delete_path(PATH_FINAL_DATASET_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0394b3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Checking Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c724cf8b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#create_path(PATH_BASE_DATA)\n",
    "#create_path(PATH_1_CREATING_DATASET)\n",
    "#create_path(PATH_FIRST_DATASET)\n",
    "create_path(PATH_FIRST_DATASET_TYPE)\n",
    "#create_path(PATH_FIRST_STATISTICS)\n",
    "create_path(PATH_FIRST_STATISTICS_TYPE)\n",
    "#create_path(PATH_2_PREPARING_DATASET)\n",
    "#create_path(PATH_CLEAN_DATASET)\n",
    "create_path(PATH_CLEAN_DATASET_TYPE)\n",
    "#create_path(PATH_CLEAN_STATISTICS)\n",
    "create_path(PATH_CLEAN_STATISTICS_TYPE)\n",
    "#create_path(PATH_3_PROCESSING_DATASET)\n",
    "#create_path(PATH_SIMILARY_ACCOUNTS)\n",
    "create_path(PATH_SIMILARY_ACCOUNTS_TYPE)\n",
    "#create_path(PATH_LABEL_COUNTER)\n",
    "create_path(PATH_LABEL_COUNTER_TYPE)\n",
    "create_path(PATH_4_CLASSIFYING_DATASET)\n",
    "\n",
    "#create_path(PATH_FINAL_DATASET_TYPE)\n",
    "#create_path(PATH_FINAL_DATASET_TYPE_TRAIN)\n",
    "#create_path(PATH_FINAL_DATASET_TYPE_TEST)\n",
    "for classe in FINAL_CLASSES:\n",
    "    create_path(os.path.join(PATH_FINAL_DATASET_TYPE_TRAIN, classe))\n",
    "    create_path(os.path.join(PATH_FINAL_DATASET_TYPE_VALIDATE, classe))\n",
    "    create_path(os.path.join(PATH_FINAL_DATASET_TYPE_TEST, classe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e5bd94",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Creating Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db82974",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f253bf67",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file_logins = read_json(os.path.join(PATH_1_CREATING_DATASET, \"logins.json\"))\n",
    "    \n",
    "def next_login(id_login, len_logins):\n",
    "    id_login += 1\n",
    "    random_sleep()\n",
    "    return id_login\n",
    "\n",
    "def get_login(id_login=0, logins=file_logins[\"Logins\"]):\n",
    "    api = None\n",
    "    logged = False\n",
    "    while not logged:\n",
    "        if not logins[id_login][\"temporaly_blocked\"]:\n",
    "            log_print(\"Trying login in account: \" + logins[id_login][\"username\"])\n",
    "            api = InstagramAPI(logins[id_login][\"username\"], logins[id_login][\"password\"])\n",
    "            logged = api.login()\n",
    "            if not logged:\n",
    "                id_login = next_login(id_login, len(logins))\n",
    "        else:\n",
    "            id_login = next_login(id_login, len(logins))\n",
    "    return (api, id_login)\n",
    "\n",
    "def get_from_api(api_function, api, id_login, params):\n",
    "\n",
    "    aux_apilastjson = \"\"\n",
    "    while True:\n",
    "        \n",
    "        if api_function == \"searchUsername\":\n",
    "            api.searchUsername(params[\"acc\"])\n",
    "        elif api_function == \"getUserFeed\":\n",
    "            if params[\"next_max_id\"] == None:\n",
    "                api.getUserFeed(params[\"user\"][\"pk\"], minTimestamp=int(DATE_BEGIN))\n",
    "            else:\n",
    "                api.getUserFeed(params[\"user\"][\"pk\"], maxid=params[\"next_max_id\"], minTimestamp=int(DATE_BEGIN))\n",
    "\n",
    "        log_print(f\"{api_function}-status_code: {api.LastResponse.status_code}\")\n",
    "        if api.LastResponse.status_code == 560 or api.LastResponse.status_code == 500:\n",
    "            log_print(\"Retry...\")\n",
    "            continue\n",
    "        elif api.LastResponse.status_code == 429 or ('feedback_message' in api.LastJson and 'We restrict certain activity to protect our community. Based on your use, this action will be unavailable for you until' in api.LastJson[\"feedback_message\"]) or ('message' in api.LastJson and api.LastJson[\"message\"] == 'challenge_required'):\n",
    "            (api, id_login) = get_login(id_login=id_login+1)\n",
    "        elif api.LastResponse.status_code != 200:\n",
    "            log_print(f\"LastResponse: {api.LastResponse}\")\n",
    "            if aux_apilastjson != api.LastJson:\n",
    "                log_print(f\"LastJson: {api.LastJson}\")\n",
    "                raise Exception(f\"{api.LastResponse.status_code}-{api.LastJson}\")\n",
    "            else:\n",
    "                raise Exception(f\"{api.LastResponse.status_code}\")\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        aux_apilastjson = api.LastJson\n",
    "        random_sleep()\n",
    "    \n",
    "    return (api, id_login)\n",
    "\n",
    "def search_username(api, id_login, acc):\n",
    "    log_print(f\"Searching username {acc}\")\n",
    "\n",
    "    params = {}\n",
    "    params[\"acc\"] = acc\n",
    "    (api, id_login) = get_from_api(\"searchUsername\", api, id_login, params)\n",
    "    \n",
    "    return (api, id_login)\n",
    "\n",
    "def create_file_user_info(api, id_login, acc):\n",
    "    \n",
    "    (api, id_login) = search_username(api, id_login, acc)\n",
    "    \n",
    "    imgs_user = {}\n",
    "    imgs_user[\"user\"] = acc\n",
    "    imgs_user[\"downloaded\"] = 0\n",
    "\n",
    "    user = api.LastJson[\"user\"]\n",
    "    \n",
    "    if user[\"follower_count\"] >= MIN_FOLLOWERS:\n",
    "        create_path(os.path.join(PATH_FIRST_DATASET_TYPE, acc))\n",
    "        log_print(\"Creating file user_info\")\n",
    "        user[\"collected_at\"] = datetime.timestamp(datetime.now())\n",
    "        write_json(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"user_info.json\"), user)\n",
    "        log_print(\"File user_info created\")\n",
    "    else:\n",
    "        raise Exception(f\"User has fewer followers than {MIN_FOLLOWERS}\")\n",
    "    \n",
    "    return (api, id_login, user,)\n",
    "\n",
    "def get_user_feed(api, id_login, user, next_max_id=None):  \n",
    "    \n",
    "    params = {}\n",
    "    params[\"next_max_id\"] = next_max_id\n",
    "    params[\"user\"] = user\n",
    "    (api, id_login) = get_from_api(\"getUserFeed\", api, id_login, params)\n",
    "    \n",
    "    return (api, id_login)\n",
    "\n",
    "def create_file_user_posts(api, id_login, user):\n",
    "    log_print(\"Creating file user_posts\")\n",
    "    (api, id_login) = get_user_feed(api, id_login, user)\n",
    "        \n",
    "    posts = api.LastJson\n",
    "\n",
    "    while (posts[\"more_available\"]):\n",
    "        random_sleep()\n",
    "        aux_bool = False\n",
    "        (api, id_login) = get_user_feed(api, id_login, user, posts[\"next_max_id\"])\n",
    "\n",
    "        posts[\"items\"].extend(api.LastJson[\"items\"])\n",
    "        posts[\"num_results\"] += api.LastJson[\"num_results\"]\n",
    "        posts[\"more_available\"] =  api.LastJson[\"more_available\"]\n",
    "        if posts[\"more_available\"]:\n",
    "            posts[\"next_max_id\"] = api.LastJson[\"next_max_id\"]\n",
    "        else:\n",
    "            del posts['next_max_id']\n",
    "        posts[\"auto_load_more_enabled\"] = api.LastJson[\"auto_load_more_enabled\"]\n",
    "        posts[\"status\"] = api.LastJson[\"status\"]\n",
    "        \n",
    "    posts[\"collected_at\"] = datetime.timestamp(datetime.now())\n",
    "    if posts[\"num_results\"] > 0:\n",
    "        write_json(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"user_posts.json\"), posts)\n",
    "        log_print(\"File user_posts created\")\n",
    "    else:\n",
    "        raise Exception(f\"User has 0 posts\")\n",
    "    \n",
    "    return (api, id_login, posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908fbc73",
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(PATH_1_CREATING_DATASET, f\"{TYPE}_users.bkp\")):\n",
    "    shutil.copyfile(os.path.join(PATH_1_CREATING_DATASET, f\"{TYPE}_users.txt\"), os.path.join(PATH_1_CREATING_DATASET, f\"{TYPE}_users.bkp\"))\n",
    "\n",
    "(api, id_login) = get_login()\n",
    "\n",
    "log_print(\"Type: \" + TYPE)\n",
    "log_print(\"Starting...\\n\")\n",
    "\n",
    "list_accounts = get_accounts()\n",
    "for acc in list_accounts:\n",
    "    try:\n",
    "        \n",
    "        (api, id_login, user) = create_file_user_info(api, id_login, acc)\n",
    "        (api, id_login, posts) = create_file_user_posts(api, id_login, user)\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_log(acc, \"GettingData\", e)\n",
    "    else:\n",
    "        log_print(f\"Account {acc} successfully processed\")\n",
    "        print(\"**********************************************\\n\")\n",
    "\n",
    "log_print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db1fa5c",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Creating Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2b582",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_candidate(post):\n",
    "    if CANDIDATE != \"N\": \n",
    "        candidate_aux = CANDIDATE\n",
    "        while (candidate_aux < 0 and abs(candidate_aux) > len(post[\"image_versions2\"][\"candidates\"])) or \\\n",
    "            (candidate_aux >= 0 and candidate_aux >= len(post[\"image_versions2\"][\"candidates\"])):\n",
    "            if candidate_aux > 0:\n",
    "                candidate_aux -= 1\n",
    "            elif candidate_aux < 0:\n",
    "                candidate_aux += 1\n",
    "    else:\n",
    "        candidate_aux = -1\n",
    "        while post[\"image_versions2\"][\"candidates\"][candidate_aux][\"width\"] < 500 or post[\"image_versions2\"][\"candidates\"][candidate_aux][\"height\"] < 500:\n",
    "            candidate_aux -= 1\n",
    "            if abs(candidate_aux) > len(post[\"image_versions2\"][\"candidates\"]):\n",
    "                return None\n",
    "    \n",
    "    return candidate_aux\n",
    "\n",
    "def create_file_user_posts_overview(posts):\n",
    "    \n",
    "    log_print(\"Creating file user_posts_overview\")\n",
    "    \n",
    "    media_count_total = 0\n",
    "    posts_overview = {}\n",
    "    posts_overview[\"collected_at\"] = posts[\"collected_at\"]\n",
    "    posts_overview[\"num_results\"] = posts[\"num_results\"]\n",
    "    posts_overview[\"items\"] = []\n",
    "    \n",
    "    for post in posts[\"items\"]:\n",
    "        if not post[\"media_type\"] in MEDIA_TYPES:\n",
    "            continue\n",
    "        if ONLY_WITH_COMMENTS and \"comments_disabled\" in post:\n",
    "            continue\n",
    "            \n",
    "        post_overview = {}\n",
    "        post_overview[\"pk\"] = post[\"pk\"]\n",
    "        post_overview[\"id\"] = post[\"id\"]\n",
    "        post_overview[\"taken_at\"] = post[\"taken_at\"]\n",
    "        post_overview[\"taken_at_year\"] = datetime.fromtimestamp(post_overview[\"taken_at\"]).year\n",
    "\n",
    "        post_overview[\"caption_text\"] = \"\"\n",
    "        if not post[\"caption\"] is None:\n",
    "            post_overview[\"caption_text\"] = post[\"caption\"][\"text\"]\n",
    "\n",
    "        post_overview[\"media_type\"] = post[\"media_type\"]\n",
    "        post_overview[\"images\"] = []\n",
    "        if post_overview[\"media_type\"] == 1:\n",
    "            post_overview[\"media_count\"] = 1\n",
    "            \n",
    "            candidate_aux = get_candidate(post)\n",
    "            \n",
    "            if candidate_aux == None:\n",
    "                continue\n",
    "            \n",
    "            image = {}\n",
    "            image[\"pk\"] = post_overview[\"pk\"]\n",
    "            image[\"width\"] = post[\"image_versions2\"][\"candidates\"][candidate_aux][\"width\"]\n",
    "            image[\"height\"] = post[\"image_versions2\"][\"candidates\"][candidate_aux][\"height\"]\n",
    "            image[\"url\"] = post[\"image_versions2\"][\"candidates\"][candidate_aux][\"url\"]\n",
    "            post_overview[\"images\"].append(image)\n",
    "        elif post_overview[\"media_type\"] == 8:\n",
    "            post_overview[\"media_count\"] = post[\"carousel_media_count\"]\n",
    "            for carousel_post in post[\"carousel_media\"]:\n",
    "                \n",
    "                candidate_aux = get_candidate(carousel_post)\n",
    "                \n",
    "                image = {}\n",
    "                image[\"pk\"] = carousel_post[\"pk\"]\n",
    "                image[\"width\"] = carousel_post[\"image_versions2\"][\"candidates\"][candidate_aux][\"width\"]\n",
    "                image[\"height\"] = carousel_post[\"image_versions2\"][\"candidates\"][candidate_aux][\"height\"]\n",
    "                image[\"url\"] = carousel_post[\"image_versions2\"][\"candidates\"][candidate_aux][\"url\"]\n",
    "                post_overview[\"images\"].append(image)\n",
    "\n",
    "        media_count_total += post_overview[\"media_count\"]\n",
    "        \n",
    "        post_overview[\"location\"] = None\n",
    "        if \"location\" in post:\n",
    "            post_overview[\"location\"] = post[\"location\"]\n",
    "\n",
    "        post_overview[\"like_count\"] = post[\"like_count\"]\n",
    "\n",
    "        post_overview[\"comments_disabled\"] = False\n",
    "        if \"comments_disabled\" in post:\n",
    "            post_overview[\"comments_disabled\"] = post[\"comments_disabled\"]\n",
    "        if post_overview[\"comments_disabled\"]:\n",
    "            post_overview[\"comment_count\"] = 0\n",
    "        else:\n",
    "            post_overview[\"comment_count\"] = post[\"comment_count\"]\n",
    "\n",
    "        post_overview[\"like_comment_count\"] = post_overview[\"like_count\"] + post_overview[\"comment_count\"]\n",
    "            \n",
    "        post_overview[\"caption_text_length\"] = len(post_overview[\"caption_text\"])\n",
    "        hashtags = [word[1:] for word in post_overview[\"caption_text\"].split() if word[0] == '#']\n",
    "        post_overview[\"caption_hashtags_count\"] = len(hashtags)\n",
    "        post_overview[\"caption_hashtags\"] = []\n",
    "        \n",
    "        if post_overview[\"caption_hashtags_count\"] > 0:\n",
    "            post_overview[\"caption_hashtags\"] = hashtags\n",
    "        post_overview[\"timestamp_duration\"] = posts_overview[\"collected_at\"] - post_overview[\"taken_at\"]\n",
    "        post_overview[\"likes_by_duration\"] = post_overview[\"like_count\"] / post_overview[\"timestamp_duration\"]\n",
    "        post_overview[\"comments_by_duration\"] = post_overview[\"comment_count\"] / post_overview[\"timestamp_duration\"]\n",
    "\n",
    "        posts_overview[\"items\"].append(post_overview)\n",
    "            \n",
    "    posts_overview[\"media_count_total\"] = media_count_total\n",
    "    write_json(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"user_posts_overview.json\"), posts_overview)\n",
    "    log_print(\"File user_posts_overview created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c1f7ba",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list_accounts = get_accounts()\n",
    "for acc in list_accounts:\n",
    "    \n",
    "    try:\n",
    "        log_print(f\"Processing account {acc}\")\n",
    "\n",
    "        posts = read_json(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"user_posts.json\"))\n",
    "\n",
    "        create_file_user_posts_overview(posts)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_log(acc, \"CreatingOverview\", e)\n",
    "    else:\n",
    "        log_print(f\"Account {acc} successfully processed\")\n",
    "        print(\"**********************************************\\n\")\n",
    "        \n",
    "log_print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ff5c8f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Downloading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d209d8cf",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "again = True\n",
    "while again:\n",
    "    again = False\n",
    "    list_accounts = get_accounts()\n",
    "    for acc in list_accounts:\n",
    "\n",
    "        try:\n",
    "            log_print(f\"Processing account {acc}\")\n",
    "            \n",
    "            create_path(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"images\"))\n",
    "                \n",
    "            posts_overview = read_json(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"user_posts_overview.json\"))\n",
    "\n",
    "            log_print(\"Downloading posts pictures:\")\n",
    "            imgs_user_downloaded = len(list(next(os.walk(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"images\")))[2])) - 1 #0 - root, 1 - dirs, 2 - files\n",
    "            if imgs_user_downloaded <= 0:\n",
    "                imgs_user_downloaded = 0\n",
    "            if imgs_user_downloaded < posts_overview['media_count_total']:\n",
    "                for post in posts_overview[\"items\"]:\n",
    "                    for image in post[\"images\"]:\n",
    "                        if not os.path.exists(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"images\", str(image[\"pk\"]) + \".jpg\")):\n",
    "                            download_image(image['url'], os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"images\", f\"{image['pk']}.jpg\"))\n",
    "                            imgs_user_downloaded += 1\n",
    "                            log_print(f\"{acc} - {imgs_user_downloaded}/{posts_overview['media_count_total']}\")\n",
    "                \n",
    "            log_print(f\"{imgs_user_downloaded} image(s) downloaded\")\n",
    "        except Exception as e:\n",
    "            error_log(acc, \"DownloadingImages\", e)\n",
    "            again = True\n",
    "        else:\n",
    "            log_print(f\"Account {acc} successfully processed\")\n",
    "            print(\"**********************************************\\n\")\n",
    "            \n",
    "log_print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421d04d2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427a4e7c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Remove Users Without Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea8c425",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "remove_users_without_images(PATH_1_CREATING_DATASET, \"Dataset\", \"user_posts_overview.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2bc723",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Generate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec1abf0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "statistics = generate_statistics(PATH_1_CREATING_DATASET, \"Dataset\", \"user_posts_overview.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6568102",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "statistics = generate_boxplots(statistics[\"df_statistics\"], PATH_1_CREATING_DATASET)\n",
    "log_print(\"REMOVA OS USUÁRIOS DO TXT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac9bdb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "statistics = generate_statistics(PATH_1_CREATING_DATASET, \"Dataset\", \"user_posts_overview.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4dae6d",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_histogram(statistics['df_statistics']['followers_count'].tolist(), \"Seguidores\", \"Frequência\", \"Frequência de seguidores dos usuários\", PATH_1_CREATING_DATASET, \"histogram_followers_without_outliers.png\")\n",
    "create_histogram(statistics['array_likes'], \"Curtidas\", \"Frequência\", \"Frequência de curtidas das imagens\", PATH_1_CREATING_DATASET, \"histogram_likes_without_outliers.png\", 40, (18,6))\n",
    "create_histogram(statistics['array_comments'], \"Comentários\", \"Frequência\", \"Frequência de comentários das imagens\", PATH_1_CREATING_DATASET, \"histogram_comments_without_outliers.png\", 40, (18,6))\n",
    "create_histogram(statistics['array_likes_comments'], \"Curtidas+Comentários\", \"Frequência\", \"Frequência de curtidas+comentários das imagens\", PATH_1_CREATING_DATASET, \"histogram_likes+comments_without_outliers.png\", 40, (18,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1b424e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Preparing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b554a25",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## CNN Clean Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e7262",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PATH_CNN_CLEAN_BASE                       = os.path.join(PATH_2_PREPARING_DATASET, \"CNN_Clean\")\n",
    "PATH_CNN_CLEAN_DATASET                    = os.path.join(PATH_CNN_CLEAN_BASE, \"CNN_Dataset\")\n",
    "PATH_CNN_CLEAN_DATASET_TYPE               = os.path.join(PATH_CNN_CLEAN_DATASET, TYPE)\n",
    "PATH_CNN_CLEAN_DATASET_TYPE_TRAIN         = os.path.join(PATH_CNN_CLEAN_DATASET_TYPE, \"Train\")\n",
    "PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_TYPE    = os.path.join(PATH_CNN_CLEAN_DATASET_TYPE_TRAIN, TYPE)\n",
    "PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_NONTYPE = os.path.join(PATH_CNN_CLEAN_DATASET_TYPE_TRAIN, f\"non_{TYPE}\")\n",
    "PATH_CNN_CLEAN_DATASET_TYPE_TEST          = os.path.join(PATH_CNN_CLEAN_DATASET_TYPE, \"Test\")\n",
    "PATH_CNN_CLEAN_DATASET_TYPE_TEST_TYPE     = os.path.join(PATH_CNN_CLEAN_DATASET_TYPE_TEST, TYPE)\n",
    "PATH_CNN_CLEAN_DATASET_TYPE_TEST_NONTYPE  = os.path.join(PATH_CNN_CLEAN_DATASET_TYPE_TEST, f\"non_{TYPE}\")\n",
    "PATH_CNN_CLEAN_OUTPUT                     = os.path.join(PATH_CNN_CLEAN_BASE, \"CNN_Output\")\n",
    "PATH_CNN_CLEAN_OUTPUT_TYPE                = os.path.join(PATH_CNN_CLEAN_OUTPUT, TYPE)\n",
    "\n",
    "print(f\"PATH_CNN_CLEAN_BASE                      : {PATH_CNN_CLEAN_BASE}\")\n",
    "print(f\"PATH_CNN_CLEAN_DATASET                   : {PATH_CNN_CLEAN_DATASET}\")\n",
    "print(f\"PATH_CNN_CLEAN_DATASET_TYPE              : {PATH_CNN_CLEAN_DATASET_TYPE}\")\n",
    "print(f\"PATH_CNN_CLEAN_DATASET_TYPE_TRAIN        : {PATH_CNN_CLEAN_DATASET_TYPE_TRAIN}\")\n",
    "print(f\"PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_TYPE   : {PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_TYPE}\")\n",
    "print(f\"PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_NONTYPE: {PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_NONTYPE}\")\n",
    "print(f\"PATH_CNN_CLEAN_DATASET_TYPE_TEST         : {PATH_CNN_CLEAN_DATASET_TYPE_TEST}\")\n",
    "print(f\"PATH_CNN_CLEAN_DATASET_TYPE_TEST_TYPE    : {PATH_CNN_CLEAN_DATASET_TYPE_TEST_TYPE}\")\n",
    "print(f\"PATH_CNN_CLEAN_DATASET_TYPE_TEST_NONTYPE : {PATH_CNN_CLEAN_DATASET_TYPE_TEST_NONTYPE}\")\n",
    "print(f\"PATH_CNN_CLEAN_OUTPUT                    : {PATH_CNN_CLEAN_OUTPUT}\")\n",
    "print(f\"PATH_CNN_CLEAN_OUTPUT_TYPE               : {PATH_CNN_CLEAN_OUTPUT_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6032c957",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Reset Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a61ab",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "delete_path(PATH_CNN_CLEAN_DATASET_TYPE)\n",
    "delete_path(PATH_CNN_CLEAN_OUTPUT_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af36d5",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Checking Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67af659",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#create_path(PATH_CNN_CLEAN_BASE)\n",
    "#create_path(PATH_CNN_CLEAN_DATASET)\n",
    "#create_path(PATH_CNN_CLEAN_DATASET_TYPE)\n",
    "#create_path(PATH_CNN_CLEAN_DATASET_TYPE_TRAIN)\n",
    "create_path(PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_TYPE)\n",
    "create_path(PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_NONTYPE)\n",
    "#create_path(PATH_CNN_CLEAN_DATASET_TYPE_TEST)\n",
    "create_path(PATH_CNN_CLEAN_DATASET_TYPE_TEST_TYPE)\n",
    "create_path(PATH_CNN_CLEAN_DATASET_TYPE_TEST_NONTYPE)\n",
    "#create_path(PATH_CNN_CLEAN_OUTPUT)\n",
    "create_path(PATH_CNN_CLEAN_OUTPUT_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f81c580",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Renaming Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e46ae",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rename_images(img_list, file_path, prefix):\n",
    "    count = 0\n",
    "    for img in img_list:\n",
    "        os.rename(os.path.join(file_path, img), os.path.join(file_path, f\"{prefix}_{count}.jpg\"))\n",
    "        count += 1\n",
    "    \n",
    "train_type_images = next(os.walk(PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_TYPE))[2] #0 - root, 1 - dirs, 2 - files\n",
    "train_nontype_images = next(os.walk(PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_NONTYPE))[2] #0 - root, 1 - dirs, 2 - files\n",
    "test_type_images = next(os.walk(PATH_CNN_CLEAN_DATASET_TYPE_TEST_TYPE))[2] #0 - root, 1 - dirs, 2 - files\n",
    "test_nontype_images = next(os.walk(PATH_CNN_CLEAN_DATASET_TYPE_TEST_NONTYPE))[2] #0 - root, 1 - dirs, 2 - files\n",
    "\n",
    "rename_images(train_type_images, PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_TYPE, 1)\n",
    "rename_images(train_nontype_images, PATH_CNN_CLEAN_DATASET_TYPE_TRAIN_NONTYPE, 0)\n",
    "rename_images(test_type_images, PATH_CNN_CLEAN_DATASET_TYPE_TEST_TYPE, 1)\n",
    "rename_images(test_nontype_images, PATH_CNN_CLEAN_DATASET_TYPE_TEST_NONTYPE, 0)\n",
    "\n",
    "log_print(\"All images has been renamed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204a4625",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb9d74",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit([f\"non_{TYPE}\", TYPE])\n",
    "# serialize the label encoder to disk\n",
    "joblib.dump(le, os.path.join(PATH_CNN_CLEAN_OUTPUT_TYPE, \"le.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3406ba6d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cnn_model, target_size, final_shape = get_cnn_model(CNN_MODEL_CLEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2708030f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# loop over the data splits\n",
    "#for split in ([\"Test\", \"Train\"]):\n",
    "\n",
    "log_print(f\"Processing 'Train split'...\")\n",
    "p = os.path.join(PATH_CNN_CLEAN_DATASET_TYPE, \"Train\")\n",
    "df_data = pd.DataFrame(list(paths.list_images(p)), columns=[\"imagePath\"])\n",
    "df_data['label'] = df_data.apply(lambda row: row.imagePath.split(os.path.sep)[-2], axis=1)\n",
    "#output_path = os.path.join(PATH_CNN_CLEAN_OUTPUT_TYPE, f\"{CNN_MODEL_CLEAN}_Train_features.avro\")\n",
    "df_train = loop_images_extract_features(cnn_model, target_size, final_shape, df_data, [\"label\"], \"label\", \"output_path\")\n",
    "\n",
    "log_print(f\"=================================\")\n",
    "\n",
    "log_print(f\"Processing 'Test split'...\")\n",
    "p = os.path.join(PATH_CNN_CLEAN_DATASET_TYPE, \"Test\")\n",
    "df_data = pd.DataFrame(list(paths.list_images(p)), columns=[\"imagePath\"])\n",
    "df_data['label'] = df_data.apply(lambda row: row.imagePath.split(os.path.sep)[-2], axis=1)\n",
    "#output_path = os.path.join(PATH_CNN_CLEAN_OUTPUT_TYPE, f\"{CNN_MODEL_CLEAN}_Test_features.avro\")\n",
    "df_test = loop_images_extract_features(cnn_model, target_size, final_shape, df_data, [\"label\"])\n",
    "\n",
    "log_print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dcf398",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b47837a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93905292",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_print(\"Loading Test dataset...\")\n",
    "df_test = pdx.read_avro(os.path.join(PATH_CNN_CLEAN_OUTPUT_TYPE, f\"{CNN_MODEL_CLEAN}_Test_features.avro\"))\n",
    "#df_test = pd.read_parquet(path=os.path.join(PATH_CNN_CLEAN_OUTPUT_TYPE, f\"{CNN_MODEL_CLEAN}_Test_features\"))\n",
    "log_print(\"Test dataset loaded.\")\n",
    "\n",
    "log_print(\"Loading Train dataset...\")\n",
    "df_train = pdx.read_avro(os.path.join(PATH_CNN_CLEAN_OUTPUT_TYPE, f\"{CNN_MODEL_CLEAN}_Train_features.avro\"))\n",
    "#df_train = pd.read_parquet(path=os.path.join(PATH_CNN_CLEAN_OUTPUT_TYPE, f\"{CNN_MODEL_CLEAN}_Train_features\"))\n",
    "log_print(\"Train dataset Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76fc23b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_print(\"Loading labels...\")\n",
    "le = joblib.load(os.path.join(PATH_CNN_CLEAN_OUTPUT_TYPE, \"le.pickle\"))\n",
    "log_print(\"Labels loaded.\")\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0820c20d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d35a5b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_print(\"Generating trainX, trainY...\")\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "trainX = df_train.copy().drop(columns=[\"label\"]).to_numpy()\n",
    "trainY = np.array(le.transform(df_train.loc[:,\"label\"].copy().tolist()))\n",
    "df_train = None\n",
    "log_print(\"trainX, trainY generated.\")\n",
    "\n",
    "log_print(\"Generating testX, testY...\")\n",
    "df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "testX = df_test.copy().drop(columns=[\"label\"]).to_numpy()\n",
    "testY = np.array(le.transform(df_test.loc[:,\"label\"].copy().tolist()))\n",
    "df_test = None\n",
    "log_print(\"testX, testY generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc7151c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "classifier_model = get_classifier_model(CLASSIFIER_CLEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec62b8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_print(\"Fit...\")\n",
    "classifier_model.fit(trainX, trainY)\n",
    "log_print(\"Model trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a711d54",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_print(\"Predict...\")\n",
    "predicted = classifier_model.predict(testX)\n",
    "log_print(\"Predicted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495e85c5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_print(f\"Confusion Matrix...\")\n",
    "print(\"\")\n",
    "conf_matrix = confusion_matrix(testY, predicted, labels=[le.transform([TYPE])[0], le.transform([f\"non_{TYPE}\"])[0]])\n",
    "cmtx = pd.DataFrame(\n",
    "    conf_matrix,\n",
    "    index=[f'true: {TYPE}', f'true: non_{TYPE}'], \n",
    "    columns=[f'pred: {TYPE}', f'pred: non_{TYPE}']\n",
    ")\n",
    "print(cmtx)\n",
    "print(\"\")\n",
    "\n",
    "log_print(\"Metrics:\")\n",
    "accuracy = accuracy_score(testY, predicted)\n",
    "log_print(\"accuracy:    %.2f\" % accuracy)\n",
    "\n",
    "precision = precision_score(testY, predicted)\n",
    "log_print(\"precision:   %.2f\" % precision)\n",
    "\n",
    "recall = recall_score(testY, predicted, pos_label=le.transform([TYPE])[0])\n",
    "log_print(\"recall:      %.2f\" % recall)\n",
    "\n",
    "f1 = f1_score(testY, predicted)\n",
    "log_print(\"f1:          %.2f\" % f1)\n",
    "\n",
    "print(\"\")\n",
    "log_print(\"Confusion Matrix Normalized...\")\n",
    "print(\"\")\n",
    "conf_matrix = confusion_matrix(testY, predicted, labels=[le.transform([TYPE])[0], le.transform([f\"non_{TYPE}\"])[0]], normalize='true')\n",
    "cmtx = pd.DataFrame(\n",
    "    conf_matrix,\n",
    "    index=[f'true: {TYPE}', f'true: non_{TYPE}'], \n",
    "    columns=[f'pred: {TYPE}', f'pred: non_{TYPE}']\n",
    ")\n",
    "print(cmtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84420153",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_print(\"Saving model...\")\n",
    "joblib.dump(classifier_model, os.path.join(PATH_CNN_CLEAN_OUTPUT_TYPE, \"model.pickle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57beee6a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e9dff8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "testX = None\n",
    "textY = None\n",
    "trainX = None\n",
    "trainY = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6251ae",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def process_image(row):\n",
    "    retry = 1\n",
    "    while retry <= 3:\n",
    "        try:\n",
    "            retry += 1\n",
    "            img = load_img(row[\"path_image\"], target_size=(224, 224))\n",
    "            img = img_to_array(img)\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "            img = imagenet_utils.preprocess_input(img)\n",
    "            return img\n",
    "        except Exception as e:\n",
    "            log_print(f\"Retrying ({retry}): Downloading {row['url']}\")\n",
    "            download_image(row[\"url\"], row[\"path_image\"])\n",
    "    raise Exception(f\"{row['path_image']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca703a47",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "le = joblib.load(os.path.join(PATH_CNN_CLEAN_OUTPUT_TYPE, \"le.pickle\"))\n",
    "model = joblib.load(os.path.join(PATH_CNN_CLEAN_OUTPUT_TYPE, \"model.pickle\"))\n",
    "\n",
    "list_accounts = get_accounts()\n",
    "\n",
    "log_print(\"Loading network...\")\n",
    "vgg16 = tf.keras.applications.VGG16(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "for acc in list_accounts:\n",
    "\n",
    "    log_print(f\"Processing account: {acc}\")\n",
    "\n",
    "    try:\n",
    "    \n",
    "        posts_overview = read_json(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"user_posts_overview.json\"))\n",
    "        if len(posts_overview[\"items\"]) > 0:\n",
    "            df_posts_overview = None\n",
    "            df_posts_overview = pd.DataFrame(posts_overview[\"items\"])\n",
    "            df_posts_overview = df_posts_overview.apply(lambda x: [ (x[\"pk\"], image[\"pk\"], os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"images\", f\"{image['pk']}.jpg\"), image[\"url\"]) for image in x[\"images\"]], axis=1)\n",
    "            df_posts_overview = pd.DataFrame(list(itertools.chain.from_iterable(df_posts_overview.to_list())), columns=[\"pk_item\", \"pk_image\", \"path_image\", \"url\"])\n",
    "\n",
    "            tam = len(df_posts_overview.index)\n",
    "            for offset in range(0, tam, BATCH_SIZE):\n",
    "                log_print(f\"Batch {round(((offset+1)/BATCH_SIZE)+0.5)}/{round((tam/BATCH_SIZE)+0.5)}\")\n",
    "\n",
    "                log_print(\"Creating image_values\")\n",
    "                df_posts_overview.loc[offset:offset+BATCH_SIZE,'image_values'] = df_posts_overview.apply(lambda row: process_image(row), axis=1)\n",
    "\n",
    "                log_print(\"Predicting\")\n",
    "                batchImages = np.vstack(df_posts_overview.loc[offset:offset+BATCH_SIZE,'image_values'])\n",
    "                features = vgg16.predict(batchImages, batch_size=BATCH_SIZE)\n",
    "                features = features.reshape((features.shape[0], 7 * 7 * 512))\n",
    "                resul = model.predict(features)\n",
    "                df_posts_overview.loc[offset:offset+BATCH_SIZE,'predicted_label'] = resul\n",
    "\n",
    "                if PRINT_IMAGE_AND_PREDICTED_LABELS:\n",
    "                    log_print(\"Showing images and predicted labels:\")\n",
    "                    for index, row in df_posts_overview.loc[offset:offset+BATCH_SIZE].iterrows():\n",
    "                        if DISPLAY_IMAGE_LABEL == \"ALL\" or DISPLAY_IMAGE_LABEL == le.classes_[int(row['predicted_label'])]:\n",
    "                            display(Image(filename=row['path_image']))\n",
    "                        log_print(f\"Path: {row['path_image']}\")\n",
    "                        log_print(f\"Label: {le.classes_[int(row['predicted_label'])]}\") \n",
    "                    print(\"**********************************************\\n\")\n",
    "\n",
    "            log_print(\"Creating user_posts_overview_clean.json\") \n",
    "\n",
    "            posts_overview_clean = {}\n",
    "            posts_overview_clean[\"collected_at\"] = posts_overview[\"collected_at\"]\n",
    "            posts_overview_clean[\"item_count\"] = 0\n",
    "            posts_overview_clean[\"items\"] = []\n",
    "            posts_overview_clean[\"image_count_total\"] = 0\n",
    "\n",
    "            for index, row in df_posts_overview.iterrows():\n",
    "                if le.classes_[int(row['predicted_label'])] == TYPE:\n",
    "                    for item in posts_overview[\"items\"]:\n",
    "                        if row['pk_item'] == item[\"pk\"]:\n",
    "                            item_copy = item.copy()\n",
    "                            item_copy[\"images\"] = []\n",
    "                            item_copy[\"image_count\"] = 0\n",
    "\n",
    "                            for image in item[\"images\"]:\n",
    "                                if row['pk_image'] == image[\"pk\"]:\n",
    "                                    item_copy[\"images\"].append(image)\n",
    "                                    item_copy[\"image_count\"] += 1\n",
    "                                    posts_overview_clean[\"image_count_total\"] += 1\n",
    "                                    break\n",
    "\n",
    "                            if item_copy[\"image_count\"] > 0:\n",
    "                                posts_overview_clean[\"items\"].append(item_copy)\n",
    "                                posts_overview_clean[\"item_count\"] += 1\n",
    "\n",
    "                            break\n",
    "\n",
    "            if posts_overview_clean[\"image_count_total\"] > 0:\n",
    "                create_path(os.path.join(PATH_CLEAN_DATASET_TYPE, acc))\n",
    "                write_json(os.path.join(PATH_CLEAN_DATASET_TYPE, acc, \"user_posts_overview_clean.json\"), posts_overview_clean)\n",
    "    except Exception as e:\n",
    "        error_log(acc, \"Cleaning\", e)\n",
    "    print(\"**********************************************\\n\")\n",
    "log_print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef227b8",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Moving Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dbc4aa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list_accounts = get_accounts()\n",
    "\n",
    "for acc in list_accounts:\n",
    "    \n",
    "    try:\n",
    "        log_print(f\"Processing account {acc}\")\n",
    "\n",
    "        shutil.copyfile(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"user_info.json\"), os.path.join(PATH_CLEAN_DATASET_TYPE, acc, \"user_info.json\"))\n",
    "\n",
    "        create_path(os.path.join(PATH_CLEAN_DATASET_TYPE, acc, \"images\"))\n",
    "\n",
    "        overview_clean = read_json(os.path.join(PATH_CLEAN_DATASET_TYPE, acc, \"user_posts_overview_clean.json\"))\n",
    "\n",
    "        images_moved = len(list(next(os.walk(os.path.join(PATH_CLEAN_DATASET_TYPE, acc, \"images\")))[2])) - 1 #0 - root, 1 - dirs, 2 - files\n",
    "        if images_moved <= 0:\n",
    "            images_moved = 0\n",
    "        for item in overview_clean[\"items\"]:\n",
    "            for image in item[\"images\"]:\n",
    "                if not os.path.exists(os.path.join(PATH_CLEAN_DATASET_TYPE, acc, \"images\", str(image[\"pk\"]) + \".jpg\")):\n",
    "                    shutil.move(os.path.join(PATH_FIRST_DATASET_TYPE, acc, \"images\", str(image[\"pk\"]) + \".jpg\"), os.path.join(PATH_CLEAN_DATASET_TYPE, acc, \"images\", str(image[\"pk\"]) + \".jpg\"))\n",
    "                    images_moved += 1\n",
    "                    log_print(f\"{acc}: {images_moved}/{overview_clean['image_count_total']}\") \n",
    "        print(\"**********************************************\\n\")\n",
    "    except Exception as e:\n",
    "        error_log(acc, \"MovingImages\", e)\n",
    "            \n",
    "log_print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21751ae2",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05497617",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Remove Users Without Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70783cb2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "remove_users_without_images(PATH_2_PREPARING_DATASET, \"Clean_Dataset\", \"user_posts_overview_clean.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acfb67e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Generate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1874929f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "statistics = generate_statistics(PATH_2_PREPARING_DATASET, \"Clean_Dataset\", \"user_posts_overview_clean.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718f3de1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "statistics['df_statistics'].to_csv(os.path.join(PATH_2_PREPARING_DATASET, \"Statistics\", TYPE, \"statistics_after_clean.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4425524",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "create_histogram(statistics['df_statistics']['followers_count'].tolist(), \"Seguidores\", \"Frequência\", \"Frequência de seguidores dos usuários após limpeza\", PATH_2_PREPARING_DATASET, \"histogram_followers_after_clean.png\")\n",
    "create_histogram(statistics['array_likes'], \"Curtidas\", \"Frequência\", \"Frequência de curtidas das imagens após limpeza\", PATH_2_PREPARING_DATASET, \"histogram_likes_after_clean.png\", 40, (18,6))\n",
    "create_histogram(statistics['array_comments'], \"Comentários\", \"Frequência\", \"Frequência de comentários das imagens após limpeza\", PATH_2_PREPARING_DATASET, \"histogram_comments_after_clean.png\", 40, (18,6))\n",
    "create_histogram(statistics['array_likes_comments'], \"Curtidas+Comentários\", \"Frequência\", \"Frequência de curtidas+comentários das imagens após limpeza\", PATH_2_PREPARING_DATASET, \"histogram_likes+comments_after_clean.png\", 40, (18,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b864ca3a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Processing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a93e046",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Similary Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef7d4d0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_users_comparison(i_followers, j_followers):\n",
    "    if i_followers >= j_followers:\n",
    "        diff_radius = i_followers * FOLLOWERS_RADIUS // 100\n",
    "    else:\n",
    "        diff_radius = j_followers * FOLLOWERS_RADIUS // 100\n",
    "\n",
    "    diff_abs = abs(i_followers - j_followers)\n",
    "    \n",
    "    return { \"comparison\": diff_abs <= diff_radius,\n",
    "           \"diff_abs\": diff_abs,\n",
    "           \"diff_radius\": diff_radius }\n",
    "    \n",
    "def get_items_comparison(i_likes, j_likes):\n",
    "    if i_likes >= j_likes:\n",
    "        diff_radius = i_likes * LIKES_RADIUS // 100\n",
    "    else:\n",
    "        diff_radius = j_likes * LIKES_RADIUS // 100        \n",
    "        \n",
    "    diff_abs = abs(i_likes - j_likes)\n",
    "    \n",
    "    if diff_abs >= diff_radius:\n",
    "        i_label = \"igual\"\n",
    "        j_label = \"igual\"\n",
    "\n",
    "        if i_likes > j_likes:\n",
    "            i_label = \"inte\"\n",
    "            j_label = \"desinte\"\n",
    "        elif i_likes < j_likes:\n",
    "            i_label = \"desinte\"\n",
    "            j_label = \"inte\"\n",
    "    else:\n",
    "        i_label = None\n",
    "        j_label = None\n",
    "    \n",
    "    return { \"comparison\": diff_abs >= diff_radius,\n",
    "           \"diff_abs\": diff_abs,\n",
    "           \"diff_radius\": diff_radius,\n",
    "           \"i_label\": i_label,\n",
    "           \"j_label\": j_label}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cde9e5",
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_accounts = get_accounts()\n",
    "tam = len(list_accounts)\n",
    "\n",
    "for i in range(tam-1):\n",
    "    \n",
    "    user_info_I = read_json(os.path.join(PATH_CLEAN_DATASET_TYPE, list_accounts[i], \"user_info.json\"))\n",
    "        \n",
    "    for j in range(i+1, tam):\n",
    "        comparisons_list = []\n",
    "\n",
    "        user_info_J = read_json(os.path.join(PATH_CLEAN_DATASET_TYPE, list_accounts[j], \"user_info.json\"))\n",
    "\n",
    "        log_print(f\"Comparando conta {i+1}-[{list_accounts[i]}] com conta {j+1}-[{list_accounts[j]}] / Total: {tam} contas.\")\n",
    "\n",
    "        users_comparison = get_users_comparison(user_info_I[\"follower_count\"], user_info_I[\"follower_count\"])\n",
    "        if users_comparison[\"comparison\"]:\n",
    "\n",
    "            user_posts_overview_clean_I = read_json(os.path.join(PATH_CLEAN_DATASET_TYPE, list_accounts[i], \"user_posts_overview_clean.json\"))\n",
    "            user_posts_overview_clean_J = read_json(os.path.join(PATH_CLEAN_DATASET_TYPE, list_accounts[j], \"user_posts_overview_clean.json\"))\n",
    "\n",
    "            random.shuffle(user_posts_overview_clean_I[\"items\"])\n",
    "            random.shuffle(user_posts_overview_clean_J[\"items\"])\n",
    "\n",
    "            for item_I in user_posts_overview_clean_I[\"items\"]:\n",
    "                for item_J in user_posts_overview_clean_J[\"items\"]:\n",
    "\n",
    "                    if abs(item_I[\"taken_at\"] - item_J[\"taken_at\"]) <= TIMESTAMP_DIFFERENCE:          \n",
    "                        items_comparison = get_items_comparison(item_I[\"like_count\"], item_J[\"like_count\"])\n",
    "                        if (items_comparison[\"comparison\"]):\n",
    "\n",
    "                            row = {\n",
    "                                 'type': TYPE,\n",
    "                                 'i_username': list_accounts[i],\n",
    "                                 'i_follower_count': user_info_I[\"follower_count\"],\n",
    "                                 'j_username': list_accounts[j],\n",
    "                                 'j_follower_count': user_info_J[\"follower_count\"],\n",
    "                                 'diff_followers_abs': users_comparison[\"diff_abs\"],\n",
    "                                 'diff_followers_radius': users_comparison[\"diff_radius\"],\n",
    "                                 'i_pk': item_I[\"images\"][0][\"pk\"],\n",
    "                                 'i_like_count': item_I[\"like_count\"],\n",
    "                                 'j_pk': item_J[\"images\"][0][\"pk\"],\n",
    "                                 'j_like_count': item_J[\"like_count\"],\n",
    "                                 'diff_likes_abs': items_comparison[\"diff_abs\"],\n",
    "                                 'diff_likes_radius': items_comparison[\"diff_radius\"],\n",
    "                                 'i_label': items_comparison[\"i_label\"],\n",
    "                                 'j_label': items_comparison[\"j_label\"]\n",
    "                            }\n",
    "\n",
    "                            comparisons_list.append(row)\n",
    "\n",
    "            if len(comparisons_list) > 0:\n",
    "                log_print(f\"Saving {len(comparisons_list)} new rows\")\n",
    "                df = pd.DataFrame(comparisons_list)\n",
    "                df.to_parquet(path=PATH_SIMILARY_ACCOUNTS_TYPE, partition_cols=\"i_username\")\n",
    "\n",
    "log_print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac35586",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Label Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da81756",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201f9e33",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!echo 1 > /proc/sys/vm/overcommit_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc9d93e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### \"inte\" and \"desinte\" in label0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e18714",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_similary_accounts = pd.read_parquet(path=PATH_SIMILARY_ACCOUNTS_TYPE)\n",
    "print(f\"Comparações: {df_similary_accounts.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb3f5e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_i = df_similary_accounts.loc[:, [\"type\", \"i_username\", \"i_follower_count\", \"i_pk\", \"i_like_count\", \"i_label\"]].copy()\n",
    "df_j = df_similary_accounts.loc[:, [\"type\", \"j_username\", \"j_follower_count\", \"j_pk\", \"j_like_count\", \"j_label\"]].copy()\n",
    "\n",
    "columns = [\"type\", \"username\", \"follower_count\", \"pk\", \"like_count\", \"label\"]\n",
    "df_i.columns = columns\n",
    "df_j.columns = columns\n",
    "\n",
    "df_united = pd.concat([df_i, df_j], axis=\"index\")\n",
    "df_i = None\n",
    "df_j = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e05e876",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_similary_accounts = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bf0ee8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_united = df_united.groupby(columns).size().reset_index(name=\"count\").sort_values([\"pk\",\"count\"], ascending=[True, False]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe5093",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# AUXILIAR PARA ENCONTRAR PKS QUE RECEBERAM LABELS DIFERENTES EM DIFERENTES COMPARAÇÕES\n",
    "\n",
    "log_print(\"Beginning...\")\n",
    "for i in df_united['pk'].tolist():\n",
    "    if df_united[df_united[\"pk\"]==i].count()[\"label\"] > 1:\n",
    "        log_print(i)\n",
    "log_print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ccddea",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "s = df_united.groupby(['type','username', 'follower_count', 'pk', 'like_count']).cumcount()\n",
    "\n",
    "df_united = df_united.set_index(['type','username', 'follower_count', 'pk', 'like_count', s]).unstack().sort_index(level=1, axis=1)\n",
    "df_united.columns = [f'{x}{y}' for x, y in df_united.columns]\n",
    "df_united = df_united.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36497444",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_united[df_united[\"label0\"] == \"igual\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a515c81",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SE EXISTIR ALGUM IGUAL\n",
    "\n",
    "if RANDOM_IGUAL:\n",
    "    df_label_counter = df_united.copy()\n",
    "    df_label_counter.loc[df_label_counter['label0'] == 'igual', 'label'] = [ random.choice([\"inte\", \"desinte\"]) for k in df_label_counter.loc[df_label_counter['label0'] == 'igual'].index ]\n",
    "else:\n",
    "    df_label_counter = df_label_counter_aux.loc[df_label_counter['label'] != 'igual'].copy()\n",
    "\n",
    "df_united = df_label_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3209aa",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Generating labelB with \"m. inte\", \"inte\", \"desinte\" and \"m. desinte\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82746988",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mediana_inte = df_united[df_united[\"label0\"] == \"inte\"][\"count0\"].median()\n",
    "mediana_desinte = df_united[df_united[\"label0\"] == \"desinte\"][\"count0\"].median()\n",
    "print(f\"mediana_inte: {mediana_inte}\")\n",
    "print(f\"mediana_desinte: {mediana_desinte}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3684d901",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_united.loc[(df_united['label0'] == \"inte\") & (df_united['count0'] >= mediana_inte), 'labelB'] = \"m.inte\"\n",
    "df_united.loc[(df_united['label0'] == \"inte\") & (df_united['count0'] <  mediana_inte), 'labelB'] = \"inte\"\n",
    "df_united.loc[(df_united['label0'] == \"desinte\") & (df_united['count0'] <  mediana_desinte), 'labelB'] = \"desinte\"\n",
    "df_united.loc[(df_united['label0'] == \"desinte\") & (df_united['count0'] >= mediana_desinte), 'labelB'] = \"m.desinte\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40210f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(f\"Total: {df_united.shape}\")\n",
    "print(\"==========================================================\")\n",
    "print(f'inte      = {df_united.loc[df_united[\"label0\"] == \"inte\"].shape}')\n",
    "print(f'desinte   = {df_united.loc[df_united[\"label0\"] == \"desinte\"].shape}')\n",
    "print(\"==========================================================\")\n",
    "print(f'm.inte    = {df_united.loc[df_united[\"labelB\"] == \"m.inte\"].shape}')\n",
    "print(f'inte      = {df_united.loc[df_united[\"labelB\"] == \"inte\"].shape}')\n",
    "print(f'desinte   = {df_united.loc[df_united[\"labelB\"] == \"desinte\"].shape}')\n",
    "print(f'm.desinte = {df_united.loc[df_united[\"labelB\"] == \"m.desinte\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75afc83",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_united.to_parquet(path=PATH_LABEL_COUNTER_TYPE, partition_cols=\"labelB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e8bb6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_united = pd.read_parquet(path=PATH_LABEL_COUNTER_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fe5cce",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_united"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d047ca",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "minte_ex = df_united[df_united[\"labelB\"] == \"m.inte\"].sample()\n",
    "print(minte_ex)\n",
    "display(Image(filename=os.path.join(PATH_CLEAN_DATASET_TYPE, minte_ex.iloc[0]['username'], 'images', str(minte_ex.iloc[0]['pk']) + \".jpg\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed8b07f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inte_ex = df_united[df_united[\"labelB\"] == \"inte\"].sample()\n",
    "print(inte_ex)\n",
    "display(Image(filename=os.path.join(PATH_CLEAN_DATASET_TYPE, inte_ex.iloc[0]['username'], 'images', str(inte_ex.iloc[0]['pk']) + \".jpg\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf97557",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "desinte_ex = df_united[df_united[\"labelB\"] == \"desinte\"].sample()\n",
    "print(desinte_ex)\n",
    "display(Image(filename=os.path.join(PATH_CLEAN_DATASET_TYPE, desinte_ex.iloc[0]['username'], 'images', str(desinte_ex.iloc[0]['pk']) + \".jpg\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aba303",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mdesinte_ex = df_united[df_united[\"labelB\"] == \"m.desinte\"].sample()\n",
    "print(mdesinte_ex)\n",
    "display(Image(filename=os.path.join(PATH_CLEAN_DATASET_TYPE, mdesinte_ex.iloc[0]['username'], 'images', str(mdesinte_ex.iloc[0]['pk']) + \".jpg\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811e5a09",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec02f96d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_images = pd.read_parquet(path=PATH_LABEL_COUNTER_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c767cc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if CLASSIFIER_FINAL != \"CNN\":\n",
    "    n_images = 1000\n",
    "elif LEN_FINAL_CLASSES == 2:\n",
    "    n_images = 3000\n",
    "elif LEN_FINAL_CLASSES == 4:\n",
    "    n_images = 1500\n",
    "print(f\"n_images: {n_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b46ab9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_minte = df_images.loc[df_images[\"labelB\"] == \"m.inte\"].sort_values(by='count0', ascending=False).iloc[0:n_images].reset_index(drop=True)\n",
    "df_inte = df_images.loc[df_images[\"labelB\"] == \"inte\"].sort_values(by='count0', ascending=False).iloc[0:n_images].reset_index(drop=True)\n",
    "df_desinte = df_images.loc[df_images[\"labelB\"] == \"desinte\"].sort_values(by='count0', ascending=False).iloc[0:n_images].reset_index(drop=True)\n",
    "df_mdesinte = df_images.loc[df_images[\"labelB\"] == \"m.desinte\"].sort_values(by='count0', ascending=False).iloc[0:n_images].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e1a9c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_minte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84479f25",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_inte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7560cb4d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_desinte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b83c65",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_mdesinte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c24d631",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_images = pd.concat([df_minte,df_inte,df_desinte,df_mdesinte]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc9376b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if CLASSIFIER_FINAL != \"CNN\":\n",
    "    df_images.to_csv(path_or_buf=os.path.join(PATH_FINAL_DATASET_TYPE,f\"{LEN_FINAL_CLASSES}_classes_CLASSIFIER_dataset.csv\"), index=False)\n",
    "else:\n",
    "    df_images.to_csv(path_or_buf=os.path.join(PATH_FINAL_DATASET_TYPE,f\"{LEN_FINAL_CLASSES}_classes_CNN_dataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbed0da",
   "metadata": {},
   "source": [
    "# Classifying Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2843abc7",
   "metadata": {},
   "source": [
    "## CNN Classify Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b38bc35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH_CNN_CLASSIFY_BASE        : D:\\IC\\tcc_insta\\data\\4-Classifying_Dataset\\CNN_Classify\n",
      "PATH_CNN_CLASSIFY_OUTPUT      : D:\\IC\\tcc_insta\\data\\4-Classifying_Dataset\\CNN_Classify\\CNN_Output\n",
      "PATH_CNN_CLASSIFY_OUTPUT_TYPE : D:\\IC\\tcc_insta\\data\\4-Classifying_Dataset\\CNN_Classify\\CNN_Output\\travel\n"
     ]
    }
   ],
   "source": [
    "PATH_CNN_CLASSIFY_BASE        = os.path.join(PATH_4_CLASSIFYING_DATASET, \"CNN_Classify\")\n",
    "PATH_CNN_CLASSIFY_OUTPUT      = os.path.join(PATH_CNN_CLASSIFY_BASE, \"CNN_Output\")\n",
    "PATH_CNN_CLASSIFY_OUTPUT_TYPE = os.path.join(PATH_CNN_CLASSIFY_OUTPUT, TYPE)\n",
    "\n",
    "print(f\"PATH_CNN_CLASSIFY_BASE        : {PATH_CNN_CLASSIFY_BASE}\")\n",
    "print(f\"PATH_CNN_CLASSIFY_OUTPUT      : {PATH_CNN_CLASSIFY_OUTPUT}\")\n",
    "print(f\"PATH_CNN_CLASSIFY_OUTPUT_TYPE : {PATH_CNN_CLASSIFY_OUTPUT_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f9c0d1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Reset Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de61f4a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "delete_path(PATH_CNN_CLASSIFY_OUTPUT_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce67173",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Checking Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d56b2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#create_path(PATH_CNN_CLASSIFY_BASE)\n",
    "#create_path(PATH_CNN_CLASSIFY_OUTPUT)\n",
    "create_path(PATH_CNN_CLASSIFY_OUTPUT_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fadabd",
   "metadata": {},
   "source": [
    "## Begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9ee6a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\IC\\\\tcc_insta\\\\data\\\\4-Classifying_Dataset\\\\CNN_Classify\\\\CNN_Output\\\\travel\\\\le_2_classes.pickle']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(FINAL_CLASSES)\n",
    "# serialize the label encoder to disk\n",
    "joblib.dump(le, os.path.join(PATH_CNN_CLASSIFY_OUTPUT_TYPE, f\"le_{LEN_FINAL_CLASSES}_classes.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22af753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_print(\"Reading...\")\n",
    "if CLASSIFIER_FINAL != \"CNN\":\n",
    "    df_images = pd.read_csv(filepath_or_buffer=os.path.join(PATH_FINAL_DATASET_TYPE, f\"{LEN_FINAL_CLASSES}_classes_CLASSIFIER_dataset.csv\"))\n",
    "else:\n",
    "    df_images = pd.read_csv(filepath_or_buffer=os.path.join(PATH_FINAL_DATASET_TYPE, f\"{LEN_FINAL_CLASSES}_classes_CNN_dataset.csv\"))\n",
    "log_print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d30f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images['imagePath'] = df_images.apply(lambda row: os.path.join(PATH_CLEAN_DATASET_TYPE, row.username, 'images', str(row.pk) + '.jpg'), axis=1)\n",
    "\n",
    "df_images = df_images.loc[:, ['imagePath', 'label0', 'labelB']].copy()\n",
    "df_images = df_images.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0bfbc5",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## CNN Final com Classificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f27a32",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4c11a2",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_model, target_size, final_shape, preprocess_func = get_cnn_model(CNN_MODEL_FINAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265d1de8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_print(\"Extracting features...\")\n",
    "df_images = loop_images_extract_features(cnn_model, target_size, final_shape, df_images, [\"label0\", \"labelB\"])\n",
    "log_print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a3644b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### CNN Final com Classificadores - Train com Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b04d69",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = df_images.copy().drop(columns=[\"label0\", \"labelB\"]).to_numpy()\n",
    "\n",
    "if LEN_FINAL_CLASSES == 2:\n",
    "    y = np.array(le.transform(df_images.loc[:,\"label0\"].copy().tolist()))\n",
    "elif LEN_FINAL_CLASSES == 4:\n",
    "    y = np.array(le.transform(df_images.loc[:,\"labelB\"].copy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15221d3d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "classifier_model = get_classifier_model(CLASSIFIER_FINAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b64d38",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_result = {}\n",
    "metrics_result[\"model\"] = []\n",
    "metrics_result[\"accuracy_score\"] = []\n",
    "metrics_result[\"precision_score\"] = []\n",
    "metrics_result[\"recall_score\"] = []\n",
    "metrics_result[\"f1_score\"] = []\n",
    "metrics_result[\"confusion_matrix\"] = []\n",
    "metrics_result[\"confusion_matrix_normalized\"] = []\n",
    "\n",
    "log_print(f\"Iniciando cross validation p/ {CV} folds...\")\n",
    "kf = KFold(n_splits=CV)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "fold = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    fold += 1  \n",
    "    print(\"\")\n",
    "    log_print(f\"=========== FOLD - {fold} ===========\")\n",
    "\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    log_print(\"Fit...\")\n",
    "    classifier_model.fit(X_train, y_train)\n",
    "    \n",
    "    log_print(\"Predict...\")\n",
    "    predicted = classifier_model.predict(X_test)\n",
    "    \n",
    "    metrics_result[\"model\"].append(classifier_model)\n",
    "    \n",
    "    #print(f\"predicted: {predicted}\")\n",
    "    #print(f\"y_test:    {y_test}\")\n",
    "    \n",
    "    print(\"\")\n",
    "    log_print(f\"Confusion Matrix...\")\n",
    "    print(\"\")\n",
    "    if LEN_FINAL_CLASSES == 2:\n",
    "        conf_matrix = confusion_matrix(y_test, predicted, labels=[le.transform([\"inte\"])[0], le.transform([\"desinte\"])[0]])\n",
    "\n",
    "        cmtx = pd.DataFrame(\n",
    "            conf_matrix,\n",
    "            index=['true: inte', 'true: desinte'], \n",
    "            columns=['pred: inte', 'pred: desinte']\n",
    "        )\n",
    "    elif LEN_FINAL_CLASSES == 4:\n",
    "        conf_matrix = confusion_matrix(y_test, predicted, labels=[le.transform([\"m.inte\"])[0], le.transform([\"inte\"])[0], le.transform([\"desinte\"])[0], le.transform([\"m.desinte\"])[0]])\n",
    "\n",
    "        cmtx = pd.DataFrame(\n",
    "            conf_matrix,\n",
    "            index=['true: m.inte', 'true: inte', 'true: desinte', 'true: m.desinte'], \n",
    "            columns=['pred: m.inte', 'pred: inte', 'pred: desinte', 'pred: m.desinte']\n",
    "        )\n",
    "    print(cmtx)\n",
    "    print(\"\")\n",
    "    metrics_result[\"confusion_matrix\"].append(conf_matrix)\n",
    "\n",
    "    log_print(\"Metrics:\")\n",
    "    accuracy = accuracy_score(y_test, predicted)\n",
    "    metrics_result[\"accuracy_score\"].append(accuracy)\n",
    "    log_print(\"accuracy:    %.2f\" % accuracy)\n",
    "    \n",
    "    if LEN_FINAL_CLASSES == 2:\n",
    "        precision = precision_score(y_test, predicted)\n",
    "    elif LEN_FINAL_CLASSES == 4:\n",
    "        precision = precision_score(y_test, predicted, average=\"macro\")\n",
    "    metrics_result[\"precision_score\"].append(precision)\n",
    "    log_print(\"precision:   %.2f\" % precision)\n",
    "    \n",
    "    if LEN_FINAL_CLASSES == 2:\n",
    "        recall = recall_score(y_test, predicted, pos_label=le.transform([\"inte\"])[0])\n",
    "    elif LEN_FINAL_CLASSES == 4:\n",
    "        recall = recall_score(y_test, predicted, average=\"macro\")\n",
    "    metrics_result[\"recall_score\"].append(recall)\n",
    "    log_print(\"recall:      %.2f\" % recall)\n",
    "    \n",
    "    if LEN_FINAL_CLASSES == 2:\n",
    "        f1 = f1_score(y_test, predicted)\n",
    "    elif LEN_FINAL_CLASSES == 4:\n",
    "        f1 = f1_score(y_test, predicted, average=\"macro\")\n",
    "    metrics_result[\"f1_score\"].append(f1)\n",
    "    log_print(\"f1:          %.2f\" % f1)\n",
    "    \n",
    "    print(\"\")\n",
    "    log_print(\"Confusion Matrix Normalized...\")\n",
    "    print(\"\")\n",
    "    if LEN_FINAL_CLASSES == 2:\n",
    "        conf_matrix = confusion_matrix(y_test, predicted, labels=[le.transform([\"inte\"])[0], le.transform([\"desinte\"])[0]], normalize='true')\n",
    "        cmtx = pd.DataFrame(\n",
    "            conf_matrix,\n",
    "            index=['true: inte', 'true: desinte'], \n",
    "            columns=['pred: inte', 'pred: desinte']\n",
    "        )\n",
    "    elif LEN_FINAL_CLASSES == 4:\n",
    "        conf_matrix = confusion_matrix(y_test, predicted, labels=[le.transform([\"m.inte\"])[0], le.transform([\"inte\"])[0], le.transform([\"desinte\"])[0], le.transform([\"m.desinte\"])[0]], normalize='true')\n",
    "        cmtx = pd.DataFrame(\n",
    "            conf_matrix,\n",
    "            index=['true: m.inte', 'true: inte', 'true: desinte', 'true: m.desinte'], \n",
    "            columns=['pred: m.inte', 'pred: inte', 'pred: desinte', 'pred: m.desinte']\n",
    "        )\n",
    "    print(cmtx)\n",
    "    print(\"\")\n",
    "    metrics_result[\"confusion_matrix_normalized\"].append(conf_matrix)\n",
    "\n",
    "print(\"\")\n",
    "log_print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b32666",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_print(f'accuracy: {np.array(metrics_result[\"accuracy_score\"]).mean()} +- {np.array(metrics_result[\"accuracy_score\"]).std()}')\n",
    "log_print(f'precision: {np.array(metrics_result[\"precision_score\"]).mean()} +- {np.array(metrics_result[\"precision_score\"]).std()}')\n",
    "log_print(f'recall: {np.array(metrics_result[\"recall_score\"]).mean()} +- {np.array(metrics_result[\"recall_score\"]).std()}')\n",
    "log_print(f'f1: {np.array(metrics_result[\"f1_score\"]).mean()} +- {np.array(metrics_result[\"f1_score\"]).std()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c94a7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if LEN_FINAL_CLASSES == 2:\n",
    "    cm_00 = []\n",
    "    cm_01 = []\n",
    "    cm_10 = []\n",
    "    cm_11 = []\n",
    "\n",
    "    for cm in metrics_result[\"confusion_matrix_normalized\"]:\n",
    "        cm_00.append(cm[0][0])\n",
    "        cm_01.append(cm[0][1])\n",
    "        cm_10.append(cm[1][0])\n",
    "        cm_11.append(cm[1][1])\n",
    "\n",
    "    log_print(f'00: {np.array(cm_00).mean()} +- {np.array(cm_00).std()}')\n",
    "    log_print(f'01: {np.array(cm_01).mean()} +- {np.array(cm_01).std()}')\n",
    "    log_print(f'10: {np.array(cm_10).mean()} +- {np.array(cm_10).std()}')\n",
    "    log_print(f'11: {np.array(cm_11).mean()} +- {np.array(cm_11).std()}')\n",
    "    \n",
    "elif LEN_FINAL_CLASSES == 4:\n",
    "    cm_00 = []\n",
    "    cm_01 = []\n",
    "    cm_02 = []\n",
    "    cm_03 = []\n",
    "    cm_10 = []\n",
    "    cm_11 = []\n",
    "    cm_12 = []\n",
    "    cm_13 = []\n",
    "    cm_20 = []\n",
    "    cm_21 = []\n",
    "    cm_22 = []\n",
    "    cm_23 = []\n",
    "    cm_30 = []\n",
    "    cm_31 = []\n",
    "    cm_32 = []\n",
    "    cm_33 = []\n",
    "\n",
    "    for cm in metrics_result[\"confusion_matrix_normalized\"]:\n",
    "        cm_00.append(cm[0][0])\n",
    "        cm_01.append(cm[0][1])\n",
    "        cm_02.append(cm[0][2])\n",
    "        cm_03.append(cm[0][3])\n",
    "        cm_10.append(cm[1][0])\n",
    "        cm_11.append(cm[1][1])\n",
    "        cm_12.append(cm[1][2])\n",
    "        cm_13.append(cm[1][3])\n",
    "        cm_20.append(cm[2][0])\n",
    "        cm_21.append(cm[2][1])\n",
    "        cm_22.append(cm[2][2])\n",
    "        cm_23.append(cm[2][3])\n",
    "        cm_30.append(cm[3][0])\n",
    "        cm_31.append(cm[3][1])\n",
    "        cm_32.append(cm[3][2])\n",
    "        cm_33.append(cm[3][3])\n",
    "\n",
    "    log_print(f'00: {np.array(cm_00).mean()} +- {np.array(cm_00).std()}')\n",
    "    log_print(f'01: {np.array(cm_01).mean()} +- {np.array(cm_01).std()}')\n",
    "    log_print(f'02: {np.array(cm_02).mean()} +- {np.array(cm_02).std()}')\n",
    "    log_print(f'03: {np.array(cm_03).mean()} +- {np.array(cm_03).std()}')\n",
    "    log_print(f'10: {np.array(cm_10).mean()} +- {np.array(cm_10).std()}')\n",
    "    log_print(f'11: {np.array(cm_11).mean()} +- {np.array(cm_11).std()}')\n",
    "    log_print(f'12: {np.array(cm_12).mean()} +- {np.array(cm_12).std()}')\n",
    "    log_print(f'13: {np.array(cm_13).mean()} +- {np.array(cm_13).std()}')\n",
    "    log_print(f'20: {np.array(cm_20).mean()} +- {np.array(cm_20).std()}')\n",
    "    log_print(f'21: {np.array(cm_21).mean()} +- {np.array(cm_21).std()}')\n",
    "    log_print(f'22: {np.array(cm_22).mean()} +- {np.array(cm_22).std()}')\n",
    "    log_print(f'23: {np.array(cm_23).mean()} +- {np.array(cm_23).std()}')\n",
    "    log_print(f'30: {np.array(cm_30).mean()} +- {np.array(cm_30).std()}')\n",
    "    log_print(f'31: {np.array(cm_31).mean()} +- {np.array(cm_31).std()}')\n",
    "    log_print(f'32: {np.array(cm_32).mean()} +- {np.array(cm_32).std()}')\n",
    "    log_print(f'33: {np.array(cm_33).mean()} +- {np.array(cm_33).std()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15a35d9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Salvando\n",
    "joblib.dump(classifier_model, os.path.join(PATH_CNN_CLASSIFY_OUTPUT_TYPE,f\"model_{LEN_FINAL_CLASSES}_classes_{CNN_MODEL_FINAL}_{CLASSIFIER_FINAL}.pickle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38fc441",
   "metadata": {},
   "source": [
    "## CNN apenas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a34731d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Split Train/Test and Copy Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b220f087",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_minte_aux = train_test_split(df_images[df_images[\"labelB\"] == \"m.inte\"], test_size=0.3, shuffle=True)\n",
    "df_inte_aux = train_test_split(df_images[df_images[\"labelB\"] == \"inte\"], test_size=0.3, shuffle=True)\n",
    "df_desinte_aux = train_test_split(df_images[df_images[\"labelB\"] == \"desinte\"], test_size=0.3, shuffle=True)\n",
    "df_mdesinte_aux = train_test_split(df_images[df_images[\"labelB\"] == \"m.desinte\"], test_size=0.3, shuffle=True)\n",
    "\n",
    "df_minte_train = df_minte_aux[0]\n",
    "df_inte_train = df_inte_aux[0]\n",
    "df_desinte_train = df_desinte_aux[0]\n",
    "df_mdesinte_train = df_mdesinte_aux[0]\n",
    "\n",
    "df_minte_aux = train_test_split(df_minte_aux[1], test_size=1/3, shuffle=True)\n",
    "df_inte_aux = train_test_split(df_inte_aux[1], test_size=1/3, shuffle=True)\n",
    "df_desinte_aux = train_test_split(df_desinte_aux[1], test_size=1/3, shuffle=True)\n",
    "df_mdesinte_aux = train_test_split(df_mdesinte_aux[1], test_size=1/3, shuffle=True)\n",
    "\n",
    "df_minte_validate = df_minte_aux[0]\n",
    "df_inte_validate = df_inte_aux[0]\n",
    "df_desinte_validate = df_desinte_aux[0]\n",
    "df_mdesinte_validate = df_mdesinte_aux[0]\n",
    "\n",
    "df_minte_test = df_minte_aux[1]\n",
    "df_inte_test = df_inte_aux[1]\n",
    "df_desinte_test = df_desinte_aux[1]\n",
    "df_mdesinte_test = df_mdesinte_aux[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb6b854",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if LEN_FINAL_CLASSES == 2:  \n",
    "    df_inte_train = df_minte_train\n",
    "    df_inte_validate = df_minte_validate\n",
    "    df_inte_test = df_minte_test\n",
    "    \n",
    "    df_desinte_train = df_mdesinte_train\n",
    "    df_desinte_validate = df_mdesinte_validate\n",
    "    df_desinte_test = df_mdesinte_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1cc910",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# CÓPIA\n",
    "\n",
    "if LEN_FINAL_CLASSES == 2:\n",
    "    log_print(\"Copying 'Train'...\")\n",
    "    for path in df_inte_train[\"imagePath\"]:\n",
    "        shutil.copy(path, os.path.join(PATH_FINAL_DATASET_TYPE_TRAIN, \"inte\", path.split(os.path.sep)[-1]))\n",
    "    for path in df_desinte_train[\"imagePath\"]:\n",
    "        shutil.copy(path, os.path.join(PATH_FINAL_DATASET_TYPE_TRAIN, \"desinte\", path.split(os.path.sep)[-1]))\n",
    "\n",
    "    log_print(\"Copying 'Validate'...\")\n",
    "    for path in df_inte_validate[\"imagePath\"]:\n",
    "        shutil.copy(path, os.path.join(PATH_FINAL_DATASET_TYPE_VALIDATE, \"inte\", path.split(os.path.sep)[-1]))\n",
    "    for path in df_desinte_validate[\"imagePath\"]:\n",
    "        shutil.copy(path, os.path.join(PATH_FINAL_DATASET_TYPE_VALIDATE, \"desinte\", path.split(os.path.sep)[-1]))\n",
    "        \n",
    "    log_print(\"Copying 'Test'...\")\n",
    "    for path in df_inte_test[\"imagePath\"]:\n",
    "        shutil.copy(path, os.path.join(PATH_FINAL_DATASET_TYPE_TEST, \"inte\", path.split(os.path.sep)[-1]))\n",
    "    for path in df_desinte_test[\"imagePath\"]:\n",
    "        shutil.copy(path, os.path.join(PATH_FINAL_DATASET_TYPE_TEST, \"desinte\", path.split(os.path.sep)[-1]))\n",
    "\n",
    "elif LEN_FINAL_CLASSES == 4:\n",
    "    log_print(\"Copying 'Train'...\")\n",
    "    for path in df_minte_train[\"imagePath\"]:\n",
    "        shutil.copy(path, os.path.join(PATH_FINAL_DATASET_TYPE_TRAIN, \"m.inte\", path.split(os.path.sep)[-1]))\n",
    "    for path in df_inte_train[\"imagePath\"]:\n",
    "        shutil.copy(path, os.path.join(PATH_FINAL_DATASET_TYPE_TRAIN, \"inte\", path.split(os.path.sep)[-1]))\n",
    "    for path in df_desinte_train[\"imagePath\"]:\n",
    "        shutil.copy(path, os.path.join(PATH_FINAL_DATASET_TYPE_TRAIN, \"desinte\", path.split(os.path.sep)[-1]))\n",
    "    for path in df_mdesinte_train[\"imagePath\"]:\n",
    "        shutil.copy(path, os.path.join(PATH_FINAL_DATASET_TYPE_TRAIN, \"m.desinte\", path.split(os.path.sep)[-1]))\n",
    "        \n",
    "    log_print(\"Copying 'Validate'...\")\n",
    "    for path in df_minte_validate[\"imagePath\"]:\n",
    "        shutil.copy(path, os.path.join(PATH_FINAL_DATASET_TYPE_VALIDATE, \"m.inte\", path.split(os.path.sep)[-1]))\n",
    "    for path in df_inte_validate[\"imagePath\"]:\n",
    "        shutil.copy(path, os.path.join(PATH_FINAL_DATASET_TYPE_VALIDATE, \"inte\", path.split(os.path.sep)[-1]))\n",
    "    for path in df_desinte_validate[\"imagePath\"]:\n",
    "        shutil.copy(path, os.path.join(PATH_FINAL_DATASET_TYPE_VALIDATE, \"desinte\", path.split(os.path.sep)[-1]))\n",
    "    for path in df_mdesinte_validate[\"imagePath\"]:\n",
    "        shutil.copy(path, os.path.join(PATH_FINAL_DATASET_TYPE_VALIDATE, \"m.desinte\", path.split(os.path.sep)[-1]))\n",
    "\n",
    "    log_print(\"Copying 'Test'...\")\n",
    "    for path in df_minte_test[\"imagePath\"]:\n",
    "        shutil.copy(path, os.path.join(PATH_FINAL_DATASET_TYPE_TEST, \"m.inte\", path.split(os.path.sep)[-1]))\n",
    "    for path in df_inte_test[\"imagePath\"]:\n",
    "        shutil.copy(path, os.path.join(PATH_FINAL_DATASET_TYPE_TEST, \"inte\", path.split(os.path.sep)[-1]))\n",
    "    for path in df_desinte_test[\"imagePath\"]:\n",
    "        shutil.copy(path, os.path.join(PATH_FINAL_DATASET_TYPE_TEST, \"desinte\", path.split(os.path.sep)[-1]))\n",
    "    for path in df_mdesinte_test[\"imagePath\"]:\n",
    "        shutil.copy(path, os.path.join(PATH_FINAL_DATASET_TYPE_TEST, \"m.desinte\", path.split(os.path.sep)[-1]))\n",
    "\n",
    "log_print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7059a0",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca1bdd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-10 00:14:08 - CNN_MODEL: VGG16\n",
      "2021-11-10 00:14:08 - Loading network...\n",
      "2021-11-10 00:14:09 - Network loaded.\n",
      "2021-11-10 00:14:09 - Nova configuração para o modelo...\n",
      "2021-11-10 00:14:09 - Adiciona apos x uma camada densa com 2 neuronios (2 classes) com funcao de ativacao softmax (distribuicao de probabilidade)...\n",
      "2021-11-10 00:14:09 - Definindo modelo final...\n",
      "2021-11-10 00:14:09 - Congelando os neuronios já treinados na ImageNet...\n",
      "2021-11-10 00:14:09 - Iniciando objeto que apanhara todas as imagens de treino, processando as imagens com o metodo da VGG16...\n",
      "2021-11-10 00:14:09 - Iniciando objeto que apanhara todas as imagens de validação, processando as imagens com o metodo da VGG16...\n",
      "2021-11-10 00:14:09 - Iniciando objeto que apanhara todas as imagens de teste, processando as imagens com o metodo da VGG16...\n",
      "2021-11-10 00:14:09 - Finished.\n"
     ]
    }
   ],
   "source": [
    "#carrega o modelo da inception_v3 com os pesos aprendidos no treino da ImageNet sem a camada densa (include_top=False)\n",
    "base_model, target_size, final_shape, preprocess_func = get_cnn_model(CNN_MODEL_FINAL)\n",
    "\n",
    "#O restante do modelo e suas camadas são discutidos a seguir\n",
    "#x recebe o final da inception_v3\n",
    "\n",
    "x=base_model.output\n",
    "\n",
    "log_print(\"Nova configuração para o modelo...\")\n",
    "\n",
    "#adiciona apos x uma camada AveragePooling2D e atribui este no a x novamente (logo x e o topo novamente)\n",
    "x=tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x=tf.keras.layers.Dense(128,activation='relu', kernel_initializer=tf.keras.initializers.HeNormal())(x)\n",
    "x=tf.keras.layers.Dense(64,activation='relu')(x)\n",
    "x=tf.keras.layers.Dense(32,activation='relu')(x)\n",
    "x=tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "log_print(f\"Adiciona apos x uma camada densa com {LEN_FINAL_CLASSES} neuronios ({LEN_FINAL_CLASSES} classes) com funcao de ativacao softmax (distribuicao de probabilidade)...\")\n",
    "preds=tf.keras.layers.Dense(LEN_FINAL_CLASSES,activation='softmax')(x)\n",
    "\n",
    "log_print(\"Definindo modelo final...\")\n",
    "model=tf.keras.models.Model(inputs=base_model.input,outputs=preds)\n",
    "\n",
    "#mostrando modelo final e sua estrutura\n",
    "#model.summary()\n",
    "\n",
    "log_print(\"Congelando os neuronios já treinados na ImageNet...\") #queremos retreinar somente a ultima camada\n",
    "for l in model.layers:\n",
    "  if l.name.split('_')[0] != 'dense':\n",
    "    l.trainable=False\n",
    "  else:\n",
    "    l.trainable=True\n",
    "    \n",
    "log_print(f\"Iniciando objeto que apanhara todas as imagens de treino, processando as imagens com o metodo da {CNN_MODEL_FINAL}...\")   #rescale = 1./255., width_shift_range = 0.1, height_shift_range = 0.1, shear_range = 0.1, zoom_range = 0.1,\n",
    "train_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_func, rotation_range = 30, horizontal_flip = True)\n",
    "\n",
    "log_print(f\"Iniciando objeto que apanhara todas as imagens de validação, processando as imagens com o metodo da {CNN_MODEL_FINAL}...\")\n",
    "validate_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_func, rotation_range = 30, horizontal_flip = True)\n",
    "\n",
    "log_print(f\"Iniciando objeto que apanhara todas as imagens de teste, processando as imagens com o metodo da {CNN_MODEL_FINAL}...\")\n",
    "test_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_func)\n",
    "\n",
    "log_print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e687235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def check_units(y_true, y_pred):\n",
    "    if y_pred.shape[1] != 1:\n",
    "        y_pred = y_pred[:,1:2]\n",
    "        y_true = y_true[:,1:2]\n",
    "    return y_true, y_pred\n",
    "\n",
    "def precision_func(y_true, y_pred):\n",
    "    y_true, y_pred = check_units(y_true, y_pred)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall_func(y_true, y_pred):\n",
    "    y_true, y_pred = check_units(y_true, y_pred)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1_func(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    y_true, y_pred = check_units(y_true, y_pred)\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b087205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-10 00:14:11 - CARREGANDO PRÓPRIO DATASET PARA USO...\n",
      "Found 4200 images belonging to 2 classes.\n",
      "Found 1200 images belonging to 2 classes.\n",
      "Found 600 images belonging to 2 classes.\n",
      "2021-11-10 00:14:11 - Finished.\n"
     ]
    }
   ],
   "source": [
    "log_print(\"CARREGANDO PRÓPRIO DATASET PARA USO...\")\n",
    "\n",
    "#definindo gerador de imagens de treino\n",
    "train_generator = train_data_gen.flow_from_directory(PATH_FINAL_DATASET_TYPE_TRAIN,\n",
    "                                                 target_size=target_size, # tamanho da imagem para o generator\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical', #categorical\n",
    "                                                 shuffle=True)\n",
    "\n",
    "#definindo gerador de imagens de validação\n",
    "validate_generator = validate_data_gen.flow_from_directory(PATH_FINAL_DATASET_TYPE_VALIDATE,\n",
    "                                                 target_size=target_size, # tamanho da imagem para o generator\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical', #categorical\n",
    "                                                 shuffle=True)\n",
    "\n",
    "#definindo gerador de imagens de teste\n",
    "test_generator = test_data_gen.flow_from_directory(PATH_FINAL_DATASET_TYPE_TEST,\n",
    "                                                 target_size=target_size, # tamanho da imagem para o generator\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical', #categorical\n",
    "                                                 shuffle=False)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    mode='min')\n",
    "\n",
    "lr = tf.keras.optimizers.Adam(learning_rate=0.0001) #estabelecendo taxa de otimização\n",
    "\n",
    "                   #categorical_crossentropy\n",
    "model.compile(loss='categorical_crossentropy', optimizer=lr, metrics=['accuracy', precision_func, recall_func, f1_func])\n",
    "\n",
    "#definicao dos steps\n",
    "step_size_train = train_generator.n//train_generator.batch_size\n",
    "step_size_validate = validate_generator.n//validate_generator.batch_size\n",
    "step_size_test = test_generator.n//test_generator.batch_size\n",
    "\n",
    "log_print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b92243",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-10 00:14:11 - Treinando o modelo...\n",
      "Epoch 1/20\n",
      " 25/131 [====>.........................] - ETA: 11:03 - loss: 2.0303 - accuracy: 0.5425 - precision_func: 0.5542 - recall_func: 0.5507 - f1_func: 0.5432"
     ]
    }
   ],
   "source": [
    "log_print(\"Treinando o modelo...\")\n",
    "inicio = time.time()\n",
    "history = model.fit(train_generator,\n",
    "                   steps_per_epoch=step_size_train,\n",
    "                   epochs=EPOCHS,\n",
    "                   validation_data=validate_generator,\n",
    "                   validation_steps=step_size_validate,\n",
    "                   callbacks=[early_stopping])\n",
    "fim = time.time()\n",
    "tmp = fim-inicio\n",
    "log_print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aebc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_print('Tempo de treino e validate: ' + str(timedelta(seconds=tmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc77d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d47f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(PATH_CNN_CLASSIFY_OUTPUT_TYPE, f\"{LEN_FINAL_CLASSES}_classes_{CNN_MODEL_FINAL}_CNN_loss.svg\"))\n",
    "plt.clf()\n",
    "\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(PATH_CNN_CLASSIFY_OUTPUT_TYPE, f\"{LEN_FINAL_CLASSES}_classes_{CNN_MODEL_FINAL}_CNN_accuracy.svg\"))\n",
    "plt.clf()\n",
    "\n",
    "plt.title('Precision')\n",
    "plt.plot(history.history['precision_func'], label='train')\n",
    "plt.plot(history.history['val_precision_func'], label='validate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(PATH_CNN_CLASSIFY_OUTPUT_TYPE, f\"{LEN_FINAL_CLASSES}_classes_{CNN_MODEL_FINAL}_CNN_precision.svg\"))\n",
    "plt.clf()\n",
    "\n",
    "plt.title('Recall')\n",
    "plt.plot(history.history['recall_func'], label='train')\n",
    "plt.plot(history.history['val_recall_func'], label='validate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(PATH_CNN_CLASSIFY_OUTPUT_TYPE, f\"{LEN_FINAL_CLASSES}_classes_{CNN_MODEL_FINAL}_CNN_recall.svg\"))\n",
    "plt.clf()\n",
    "\n",
    "plt.title('F1')\n",
    "plt.plot(history.history['f1_func'], label='train')\n",
    "plt.plot(history.history['val_f1_func'], label='validate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(PATH_CNN_CLASSIFY_OUTPUT_TYPE, f\"{LEN_FINAL_CLASSES}_classes_{CNN_MODEL_FINAL}_CNN_f1.svg\"))\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae9ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_print(\"Avaliando o modelo...\")\n",
    "\n",
    "log_print(\"Validate:\")\n",
    "acc_validate = model.evaluate(validate_generator, steps=step_size_validate)\n",
    "\n",
    "log_print(\"Test:\")\n",
    "acc_test = model.evaluate(test_generator, steps=step_size_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d89e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_print(\"Predict...\")\n",
    "Y_pred = model.predict(test_generator)\n",
    "predicted = np.argmax(Y_pred, axis=1)\n",
    "y_test = test_generator.classes\n",
    "log_print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f453796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"predicted: {predicted}\")\n",
    "print(f\"y_test:    {y_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644ba915",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "log_print(f\"Confusion Matrix...\")\n",
    "print(\"\")\n",
    "if LEN_FINAL_CLASSES == 2:\n",
    "    conf_matrix = confusion_matrix(y_test, predicted, labels=[le.transform([\"inte\"])[0], le.transform([\"desinte\"])[0]])\n",
    "\n",
    "    cmtx = pd.DataFrame(\n",
    "        conf_matrix,\n",
    "        index=['true: inte', 'true: desinte'], \n",
    "        columns=['pred: inte', 'pred: desinte']\n",
    "    )\n",
    "elif LEN_FINAL_CLASSES == 4:\n",
    "    conf_matrix = confusion_matrix(y_test, predicted, labels=[le.transform([\"m.inte\"])[0], le.transform([\"inte\"])[0], le.transform([\"desinte\"])[0], le.transform([\"m.desinte\"])[0]])\n",
    "\n",
    "    cmtx = pd.DataFrame(\n",
    "        conf_matrix,\n",
    "        index=['true: m.inte', 'true: inte', 'true: desinte', 'true: m.desinte'], \n",
    "        columns=['pred: m.inte', 'pred: inte', 'pred: desinte', 'pred: m.desinte']\n",
    "    )\n",
    "print(cmtx)\n",
    "print(\"\")\n",
    "\n",
    "log_print(\"Metrics:\")\n",
    "accuracy = accuracy_score(y_test, predicted)\n",
    "log_print(\"accuracy:    %.2f\" % accuracy)\n",
    "\n",
    "if LEN_FINAL_CLASSES == 2:\n",
    "    precision = precision_score(y_test, predicted)\n",
    "elif LEN_FINAL_CLASSES == 4:\n",
    "    precision = precision_score(y_test, predicted, average=\"macro\")\n",
    "log_print(\"precision:   %.2f\" % precision)\n",
    "\n",
    "if LEN_FINAL_CLASSES == 2:\n",
    "    recall = recall_score(y_test, predicted, pos_label=le.transform([\"inte\"])[0])\n",
    "elif LEN_FINAL_CLASSES == 4:\n",
    "    recall = recall_score(y_test, predicted, average=\"macro\")\n",
    "log_print(\"recall:      %.2f\" % recall)\n",
    "\n",
    "if LEN_FINAL_CLASSES == 2:\n",
    "    f1 = f1_score(y_test, predicted)\n",
    "elif LEN_FINAL_CLASSES == 4:\n",
    "    f1 = f1_score(y_test, predicted, average=\"macro\")\n",
    "log_print(\"f1:          %.2f\" % f1)\n",
    "\n",
    "print(\"\")\n",
    "log_print(\"Confusion Matrix Normalized...\")\n",
    "print(\"\")\n",
    "if LEN_FINAL_CLASSES == 2:\n",
    "    conf_matrix = confusion_matrix(y_test, predicted, labels=[le.transform([\"inte\"])[0], le.transform([\"desinte\"])[0]], normalize='true')\n",
    "    cmtx = pd.DataFrame(\n",
    "        conf_matrix,\n",
    "        index=['true: inte', 'true: desinte'], \n",
    "        columns=['pred: inte', 'pred: desinte']\n",
    "    )\n",
    "elif LEN_FINAL_CLASSES == 4:\n",
    "    conf_matrix = confusion_matrix(y_test, predicted, labels=[le.transform([\"m.inte\"])[0], le.transform([\"inte\"])[0], le.transform([\"desinte\"])[0], le.transform([\"m.desinte\"])[0]], normalize='true')\n",
    "    cmtx = pd.DataFrame(\n",
    "        conf_matrix,\n",
    "        index=['true: m.inte', 'true: inte', 'true: desinte', 'true: m.desinte'], \n",
    "        columns=['pred: m.inte', 'pred: inte', 'pred: desinte', 'pred: m.desinte']\n",
    "    )\n",
    "print(cmtx)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a28357",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_print(\"Saving model...\")\n",
    "model.save(os.path.join(PATH_CNN_CLASSIFY_OUTPUT_TYPE, f\"{LEN_FINAL_CLASSES}_classes_{CNN_MODEL_FINAL}_CNN.h5\"), save_format=\"h5\")\n",
    "log_print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db42aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06532deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54118ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a6f264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de400e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model2 = load_model(os.path.join(PATH_CNN_CLASSIFY_OUTPUT_TYPE, f\"{LEN_FINAL_CLASSES}_classes_{CNN_MODEL_FINAL}_CNN.h5\"), custom_objects={\"precision_func\": precision_func, \"recall_func\": recall_func, \"f1_func\": f1_func})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973ba469",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21799bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predd = model2.predict(test_generator)\n",
    "predictedd = np.argmax(Y_predd, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69b407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccdc90a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "308px",
    "width": "254px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "203px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 786,
   "position": {
    "height": "40px",
    "left": "2038px",
    "right": "20px",
    "top": "120px",
    "width": "504px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
